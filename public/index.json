[{"content":"task 1 静态图 The table below shows how the UK unemployed spent their time last year.\nSummarise the information by selecting and reporting the main features, and make comparisons where relevant.\nThe table illustrates the things unemployed spent time on last year in UK.\nOverall, people prefer handling stuff in morning rather than afternoon, such as doing housework, shopping and job hunting. Whatsmore, the choice is different in some fields by gender.\nOn the one hand, 68% of unemployed intent to do housework in morning, much higher than that in the afternoon, with only 28%. Shopping is another similar circumstance, 20% declined in the afternoon compare with 46% on first half of day. However, there is a exception, both men and women tend to watch TV in the afternoon, with 10% increasing along the day.\nOn the other hand, some arrangements are varied between men and women. 49% of women spent time on housework, while only 19% men interested in that. Opposite with housework, men showed more passion than women in gardening, 27% versus 5% in a whole day. At the same time, job hunting and sitting around have little difference in gender, separately within 6% and 3%.\nEvaluation: Task Achievement: 6.0 The candidate addresses the general topic of how unemployed people in the UK spent their time last year, distinguishing between activities preferred in the morning versus the afternoon and differences between gender. However, the response lacks overall clarity in presenting precise data comparison. Some statistical details mentioned do not relate directly to the task\u0026rsquo;s requirement to discuss the table effectively. The emphasis on time slots is appropriate, but a lack of clarity in summarizing and comparing critical data lowers the score.\nCoherence and Cohesion: 6.5 The structure of the essay shows a basic logical flow, distinguishing between morning activities and gender differences. However, the transitions between ideas are sometimes abrupt, and the linking phrases like \u0026ldquo;Whatsmore\u0026rdquo; and \u0026ldquo;On the one hand\u0026rdquo; are used awkwardly. The use of paragraphs is clear, but better cohesion could be achieved by smoother transitions and a clearer linkage between the data presented and the tasks outlined in the table.\nLexical Resource: 6.0 The use of vocabulary is adequate with terms like \u0026ldquo;intent\u0026rdquo;, \u0026ldquo;declined\u0026rdquo;, and \u0026ldquo;varied\u0026rdquo; appropriately used. Some phrases and word choices are awkward or incorrect (\u0026ldquo;intent\u0026rdquo; should be \u0026ldquo;intend\u0026rdquo;, \u0026ldquo;Whatsmore\u0026rdquo; is unconventional). There is a limited range of vocabulary to describe statistical data, and some repetition of words like \u0026ldquo;morning\u0026rdquo; and \u0026ldquo;afternoon\u0026rdquo; could be replaced with synonyms to enhance lexical range.\nGrammatical Range and Accuracy: 5.5 The response includes a mix of complex and simple sentences. There are several grammatical errors (\u0026ldquo;68% of unemployed intent to do housework\u0026rdquo; should be \u0026ldquo;intend\u0026rdquo;, \u0026ldquo;separately within 6% and 3%\u0026rdquo; is unclear and should be rephrased for clarity). These mistakes hinder the clarity and accuracy of the response. Punctuation is generally handled well, but sentence structure issues detract from the overall grammatical quality.\nOverall Remarks: The response provides a basic analysis of the table\u0026rsquo;s data concerning unemployed peoples\u0026rsquo; time utilization in the UK, categorized by time of day and gender differences. Although it captures fundamental distinctions, it struggles with clarity in data presentation and accuracy in language use. The essay would benefit from fewer grammatical errors, a better range of vocabulary concerning data description, and improved logical sequencing of ideas for higher coherence. The overall grade reflects these areas of strength and needed improvements.\nOverall Grade: 6.0\nAI润色 The table illustrates how unemployed individuals in the UK spent their time last year. Overall, people tend to engage in activities more frequently in the morning than in the afternoon, such as housework, shopping, and job hunting. Furthermore, there are gender differences in activity preferences.\nOn one hand, 68% of unemployed individuals prefer doing housework in the morning, a much higher percentage compared to the afternoon, which stands at only 28%. A similar pattern is observed for shopping, with a 20% decrease in the afternoon compared to 46% in the first half of the day. However, there is an exception as both men and women tend to watch TV more in the afternoon, with a 10% increase as the day progresses.\nOn the other hand, there are variations in activity choices between men and women. For instance, 49% of women spend time on housework, whereas only 19% of men are interested in this activity. In contrast, men show a greater interest in gardening compared to women, with 27% versus 5% spending time on this activity throughout the day. Additionally, there is little difference in gender when it comes to job hunting and leisure time, with only a 6% and 3% variance, respectively.\n动态图 The graph below gives information about the percentage of the population in four Asian countries living in cities from 1970 to 2020, with predictions for 2030 and 2040.\nSummarise the information by selecting and reporting the main features, and make comparisons where relevant.\nThe line graph shows the percentage of people who live in cities in four Asian nations between 1970 and 2020, and predicts the data in 2030 and 2040.\nOverall, population in all four countires have increased more or few comparing with 50 years ago and the growing trend will be continued until 20 years later at least.\nIn 1970s, urban citizens in Thailand and Indonesia are under one fifth of total number, while the data are little bit higher in Philippines and Malaysia, around 30%. After 50 years development, population in cities of Malaysia have reached over 80%, which is the highest among the four. As Indonesia, which is the least one, raised rapidly over years and surpassed Philippines and Thailand after 2010.\nWith the prediction, the proportion of urban citizens will continuously increase after 2020, and Malaysia will keep it leading position. The urban population of all four countries will exceed a half amount in 2040.\nEvaluation: Task Achievement: 6.5 The candidate provides a clear overview of the line graph, addressing the main trends in urban population growth across four Asian nations from 1970 to 2020, along with predictions for 2030 and 2040. However, while the response mentions key data points, it lacks specific numerical details that would enhance the analysis. The statement about Malaysia having the highest urban population is accurate, but the comparison with Indonesia, the Philippines, and Thailand could be more explicitly detailed. Overall, the response captures the essence of the task but could benefit from more precise data references and a clearer summary of trends.\nCoherence and Cohesion: 6.0 The structure of the response is generally logical, with a clear progression from past data to future predictions. However, the use of cohesive devices is somewhat limited, and transitions between ideas could be smoother. Phrases like \u0026ldquo;In 1970s\u0026rdquo; and \u0026ldquo;After 50 years development\u0026rdquo; could be better integrated into the flow of the text. The candidate uses paragraphs effectively, but clearer signposting would improve the overall coherence of the response.\nLexical Resource: 6.5 The vocabulary used is appropriate for the task, with terms like \u0026ldquo;urban citizens,\u0026rdquo; \u0026ldquo;population,\u0026rdquo; and \u0026ldquo;predictions\u0026rdquo; effectively conveying the intended meaning. However, there are instances of awkward phrasing, such as \u0026ldquo;under one fifth of total number\u0026rdquo; and \u0026ldquo;little bit higher,\u0026rdquo; which detract from the overall quality. The candidate demonstrates a reasonable range of vocabulary but could enhance their lexical resource by incorporating more varied expressions and avoiding repetition.\nGrammatical Range and Accuracy: 6.0 The response includes a mix of simple and complex sentences, demonstrating a fair range of grammatical structures. However, there are several grammatical errors, such as \u0026ldquo;the population in all four countries have increased more or few\u0026rdquo; (should be \u0026ldquo;more or less\u0026rdquo;) and \u0026ldquo;the data are little bit higher\u0026rdquo; (should be \u0026ldquo;a little bit higher\u0026rdquo;). These errors affect clarity and accuracy. Punctuation is generally correct, but some sentences could be restructured for better grammatical precision.\nOverall Remarks: The response effectively summarizes the trends in urban population growth in four Asian nations, providing a clear overview of the data presented in the line graph. While it captures the main points, the analysis would benefit from more specific numerical references, improved cohesion, and greater grammatical accuracy. The overall grade reflects these strengths and areas for improvement.\nOverall Grade: 6.0\nAI润色 The line graph illustrates the percentage of urban residents in four Asian nations from 1970 to 2020, with projections for 2030 and 2040.\nOverall, the urban population has increased in all four countries compared to 50 years ago, and this upward trend is expected to continue for at least the next 20 years.\nIn the 1970s, the proportion of urban dwellers in Thailand and Indonesia was less than one-fifth of the total population, slightly higher in the Philippines and Malaysia at around 30%. Over the 50-year period, Malaysia\u0026rsquo;s urban population surged to over 80%, the highest among the four countries. Indonesia, initially the lowest, experienced rapid growth and surpassed the Philippines and Thailand after 2010.\nLooking ahead, the percentage of urban residents is projected to keep rising after 2020, with Malaysia maintaining its leading position. By 2040, the urban population in all four nations is anticipated to exceed half of the total population.\ntask 2 观点（二选一） The most important aim of science should be to improve people\u0026rsquo;s lives.\nTo what extent do you agree or disagree with this statement?\nThere is a statement claim that we should view improving people\u0026rsquo;s lives as the most important aim of science. I strongly agree with it for three aspects in economy, ethical problem and feasibility.\nFirst of all, enonomy influences both our lives and science. Over last a hundred year, human society evoluted rapidly with the assistance of science, which brings a numerous amount of fortunes to all people in the world. As the result, with the substaintial money created with high productivity, scientist have a better condition for research, which can be help them to make a further step in science.\nBesides, ethical issues should be took into consideration. Science is a double sword, if we can not estimate the influence, it will bring disaster to human. As we all know, the initial usage of bomb is for road creating through mountains. Unfortunately, it becomes a killer in war with the abuse using, brings millions of death and sorrow for family. So we must keep \u0026lsquo;improving human\u0026rsquo;s lives\u0026rsquo; as the main aim of science.\nSome people may think we should develop science as fast as we can no matter what. However, take the one under control and keep the aim on benefiting human is a more feasible way. We can gather strength and motivation effectively under a clear direction, just like driving on the express way.\nIn short, I believe the first and foremost aim of science is to benefit people. We should take science as an assistant for us rather than serving it. Science without people is nothing.\nEvaluation: Task Achievement: 6.5 The candidate strongly agrees with the statement regarding the importance of improving people\u0026rsquo;s lives as the primary goal of science. The response addresses the three aspects of economy, ethical issues, and feasibility to support this viewpoint. The candidate effectively presents a clear position and develops ideas coherently and logically. Relevant examples and evidence are provided to strengthen the argument. However, some parts of the response could be more refined to fully answer the question and avoid repetition. While the conclusion is effective, a more comprehensive analysis of the statement\u0026rsquo;s implications would enhance the score.\nCoherence and Cohesion: 6.5 The response is structured logically, with a clear introduction, body, and conclusion discussing the various aspects of the statement. Transition phrases like \u0026ldquo;First of all,\u0026rdquo; \u0026ldquo;Besides,\u0026rdquo; and \u0026ldquo;In short\u0026rdquo; help guide the reader through the argument. The paragraphs are well-organized, but some sentences could be more succinct for better cohesion. Maintaining the central theme and ensuring each paragraph has a clear topic sentence would strengthen coherence.\nLexical Resource: 6.0 The vocabulary used is generally appropriate, with terms like \u0026ldquo;fortunes,\u0026rdquo; \u0026ldquo;feasibility,\u0026rdquo; and \u0026ldquo;substantial money\u0026rdquo; effectively conveying the candidate\u0026rsquo;s ideas. However, there are instances of awkward phrasing (\u0026ldquo;evoluted rapidly\u0026rdquo;) and grammatical errors that impact the lexical quality. More precise and accurate vocabulary choices could enhance the overall lexical resource, along with increased variety and idiomatic expressions.\nGrammatical Range and Accuracy: 5.5 The response includes a mix of simple and complex sentence structures, demonstrating an attempt at grammatical variety. However, there are several grammatical errors throughout the response, such as \u0026ldquo;enonomy\u0026rdquo; (economy), \u0026ldquo;substaintial\u0026rdquo; (substantial), and \u0026ldquo;be took\u0026rdquo; (be taken), affecting clarity and accuracy. Punctuation use is adequate, but improvements in sentence structure and grammatical precision are needed for a higher score.\nOverall Remarks: The response effectively argues for considering the improvement of people\u0026rsquo;s lives as the central aim of science, with supporting points related to economy, ethics, and feasibility. While the response is well-structured and coherent, there are areas for improvement in the clarity and accuracy of language use, as well as grammatical precision. Enhancing vocabulary range and addressing grammatical errors would elevate the overall quality of the response.\nOverall Grade: 6.0\nAI润色 There is a statement that we should view improving people\u0026rsquo;s lives as the most important aim of science. I strongly agree with this for three reasons: economic impact, ethical considerations, and feasibility.\nFirst of all, the economy significantly influences both our lives and the field of science. Over the last hundred years, human society has evolved rapidly with the assistance of scientific advancements, generating substantial wealth for people worldwide. As a result, with the substantial financial resources created through high productivity, scientists have better conditions for research, enabling them to make further advancements in their fields.\nFurthermore, ethical issues must be taken into consideration. Science is a double-edged sword; if we cannot assess its consequences, it can lead to disastrous outcomes for humanity. For instance, the original purpose of explosives was to facilitate road construction through mountains. Unfortunately, these same explosives have been weaponized in warfare, leading to millions of deaths and immense sorrow for countless families. Therefore, we must prioritize \u0026ldquo;improving human lives\u0026rdquo; as the primary aim of science.\nSome may argue that we should advance science as rapidly as possible, regardless of the consequences. However, maintaining control and focusing on benefiting humanity is a more feasible approach. With a clear direction, we can effectively harness our collective strength and motivation, much like driving on a well-maintained expressway.\nIn conclusion, I believe the foremost aim of science should be to benefit people. We should view science as an ally rather than something to be served. After all, science without humanity is meaningless.\n","permalink":"http://localhost:1313/posts/iltes/%E9%9B%85%E6%80%9D%E5%86%99%E4%BD%9C/","summary":"初稿、ai评分、ai润色","title":"雅思写作练习记录"},{"content":"听力 对话中的逻辑词\n留意but、so、another、particularly等强调词，后边的很大可能是重点 技巧\n同义替换、出现原词可能是陷阱 new xxx 意味着可能有old 逻辑词、连接词 按题号顺序答题 填空题 注意几个word，有没有number 至少看空格前一个词，猜词性 看特色词（忽略重复出现的词）、专有名词、介词、数字等方便定位 看段落符号，确定逻辑关系（通过对话中连接词进行定位） 标题、小标题 地图题 方位 已知地点（图中和题目） 常识性标注（看画、小记号） 扫一遍字母 配对题 竖向比较 阅读 如何读得快：\n关键词 不要一个一个词读，一个块一起看 读不懂的读快，读得懂的认真读 逻辑词：\n转折：后边重点 并列：定位 因果 举例子：前边观点 填空题定位方法：\n同义替换 并列关系 因果关系 转折关系 主被动改写 词性 冠词、数字 从句 介绍专有名词时 判断题几种情况： True:\n同义替换 根据几句话总结 False:\n同义替换+意思相反 Not Given\n内容未提及（特例：题目中提到most、more之类的比较级，但原文并没有，也是） 提到内容，但其逻辑关系未提及 平行阅读法 适用范围：匹配题和顺序题同时出现时（一般passage2）\n定位填空题开始位置（找定位词，找不到要记住），通读文章，随读随做段落匹配，发现填空题就做并看下一个待做题。\n判断题可以看两个，避免ng\n写作 小作文 三种：图表、流程图、地图\n图表 动态图 vs 静态图\n侧重点：动态图体现趋势（change），静态图体现对比（difference） 指令：summarize the information by selecting and reporting the main features, and make comparisons where relevant.\n选择 select 描述 report 比较 compare 总体思路 通过作比较**选择出相对应的数据，基于此进行描述\n哪几方面作比较： 数值：最大值 最小值 等值 差值 倍数 大于 小于 趋势：上升 下降 波动 稳定 （只存在于动态图） 幅度：剧烈 平缓 作文结构：四段式 第一段：改写题目中的图表描述（1-2句）\n某种图shows某种数据在某种条件下\n改写三种方法：换词、换位置、概括\nbetween 19xx and 20xx ==\u0026gt; over a period of xx years 第二段：宏观概述（2-3句）\n看整体形状，动态图\u0026ndash;趋势，静态图\u0026ndash;差异 第三段：细节描述（4-5句）\n第四段：细节描述（4-5句）\n分组描述：时间、排名、自身特点、比较维度、图表数量 细节段引导词 两种不同数据：As for x, Concerning X\n相似/相反：similarly, likewise / by comparison, by contrast, on the other hand\n两张图表：as shown in the first/second chart\n逻辑连接词 并列：in addition, moreover, furthermore\n类比：similarly, likewise\n转折、对比：while, whereas, on the otherhand, by contrast, on the contrary, but, however\n描述技巧 从xxx到xxx一直是递增的\n完成进行时: have been doing till xxx (by now). （时态根据到xxx的时间） 主辅结合，一句话描述两三个数据要点\n内容组合： 前后结合（两段趋势）、文理结合（对数据的概括，避免主观）、动静结合（数据+趋势）、宏微结合（微观描述宏观比较）\n流程图 按步骤描述\n主要分类：工序流程图、生物生长过程图\n语态：\n工序：被动语态，一般现在时 生长：主动语态，一般现在时 作文结构：四段式 第一段：改写题目中的图表描述（1-2句）\n第二段：前一半流程描述（5-6句）\n第三段：后一半流程描述（5-6句）\n第四段：概述（1-2句）\n概括流程的步骤数量和概况 描述技巧 图形整体+细节\n补充文字+数字\n主辅结合：first 对A的描述, followed immediately by 对步骤B的描述\n逻辑连接词 initially, the following stage, enterting the final phase\n地图 主要分类：选址、变化\n关键词 绝对位置（东南西北）east west south north 相对位置（临近、对侧、环绕）adjacent,opposite,surround 区域的形状（长方形、三角形、L型）rectangle,triangle,L-shaped 道路的形态（直接转弯、蜿蜒曲折）direct sharp turn, winding 使用状态（使用中、废弃）in use, abandoned 植被（覆盖、空空如也）covered, barren 设施建设（建立、拆除）establish, demolish 建筑改造（扩建、缩小、移位）expand,reduce,relocate 变化类地图作文结构 第一段：改写题目中的图表描述（1-2句）\n第二段：变化前\n第三段：变化后\n按照图中丰富程度确定二三段句数 尺度描述 遵循一定顺序描述（ex.从入口由西向东） 第四段：总结（1-2句）\n概括流程的步骤数量和概况 大作文 评分 观点、逻辑、用词、语法\n分类 观点型（1-4）、分析型（5）、混搭型（6）\nDo you think this is a positive or negative development? 认为话题中的社会现象是一个好的发展趋势还是坏的发展趋势？\nTo what extent do you agree or disagree? 你在多大程度上同意、不同意话题中的观点？\nSome people think.. other people think\u0026hellip; Discuss both view and give your own opinion. 讨论话题中的两个观点，并给出自己的观点\nDo you think the advantages of this trend outweigh the disadvantages? 你认为话题中的社会现象的优点是否超过或大于他的缺点？\nWhy What How Why has this happened?\nWhat can be done to deal with this?\n针对这种社会现象，他的原因\\结果是什么？他的解决方案又是什么?\nWhat are the reasons and consequences of this trend?\n这种社会现象的原因和影响分别是什么？ 混搭 why what how + your view 时间分配 10 min 构思\n5 + 10 + 5 min 写作\n如何构思 brainstorm 写下能想到的任何观点 develop ideas in detail keep asking \u0026lsquo;why\u0026rsquo;? 想能够支撑观点的例子 归纳分组观点（编号） 观点型题目（Discuss both view）行文结构 (四段式) Introduction 转述题目 + 观点 People have different views about (opinion 1,2). While (op 1) can sometime xxx, I believe that (op 2) is more xxx. One view On the one hand, 下边单方观点的论理+论证 other view (my view) On the other hand, 论理+论证, This is the attitude that I believe (op 2). Instead of (op 1), xxxx. 驳斥op1 Conclusion 一两句话总结 In conclusion, I can understand (op1), but it seems to me that (op2) is much more xxx. 观点型题目（单方观点）行文结构 (五段式) the beginning 开头 hook + transition + points hook 预埋一些反方观点(appears xxx, simply due to xxx) It is hard to deny that (opposite detail) and such a fact leads impressionable people to generate the opinion that (opposite response) However, such a statement suffers from both logical and factual fallacies, and it should be examined meticulously. As far as (Point 1) (Point 2) and (Point 3) are concerned, I strongly hold that (our response). point1 + reasoning 论理论述 从高到低、层层递进（金字塔）high - mid - detail coherent adequate consistent (连贯、充实、切题) why? how? what? 如何写fact：follow and/or fight 进一步解释或驳斥反方观点或结合使用 optional filler（凑字数用的）：总结、假设、小例子 不要listing！ First and foremost (point 1) as (fact 1-1) For instance / to illustrate / to be more specific, (fact 1-2); In addition / not to mentione that / furthermore, (fact 1-3) (optional filler) point2 + exemplification 举例论证 例子要有普适性（个体blabla × 群体中的个体blabla √） Furthermore, the fact that (point 2 mid level) indicates that (our response) Take the case of (character), who / which (process). As a result, (result). Had it not been for (process paraphrase), (character) would never (result paraphrase). point3 + concession 驳斥反方观点 Nevertheless, a voice arises that (opposite response and point in grand opening). Ironically, (fact(s) against opposite detail). Therefore, (point3) the end 结尾 总结，可以不quote In a nutshell, I maintain that (response). Admittedly, as my favorite quote from (A famous person) goes, (person says \u0026#34;people have different opinions\u0026#34;), and some people may oppose me. However, I believe they will compromise after being exposed to my article. planning 找核心观点 -\u0026gt; 确定正反两观点 -\u0026gt; 确定立场 -\u0026gt; 提出三个论点以及一个反方论点\ntips：\n选一个好说的立场 不要拘泥于细节 分析型题目（两问）行文结构（四段式） Introduction 转述题目+承认问题 It is true that xxx. There are a variety of possible reasons for this, but steps can definitely be taken to tackle the problem. Q1 topic sentence + 论理论证 In my opinion, xxx. Firstly, Secondly, Finally... Q2 topic sentence + 论理论证 (做出回答). Start with, Also, At the same time. Conclusion 回答问题和归纳步骤 好词好句 form a virtuous cycle in xxx lift people out of poverty 口语 评分标准：流利、逻辑连贯、发音、词汇语法准确性\n","permalink":"http://localhost:1313/posts/iltes/%E9%9B%85%E6%80%9D%E6%8A%80%E5%B7%A7/","summary":"网课笔记总结","title":"雅思听说读写技巧"},{"content":"python入门 动态语言 在运行时确定变量类型\n优势：灵活、简洁、更快的开发周期 劣势：缺乏编译期类型检查容易出错 特性 动态添加属性和方法、动态类型绑定、反射和元编程\n底层实现 有一个基类承接所有对象，从而实现动态类型\n数据类型 基本类型 字符串、整数、浮点数、布尔值(True, False)、空值(None)\n一些运算符：\n// 与 / ：前者保留商的整数部分，后者是float类型的商 逻辑运算符：and or not List 声明方式：list = [1, '2', xxx]\n生成式声明：list = [expr for iter_var in iterable if cond_expr]\nList中可以存各种类型的元素\n相关操作 List截取与go中切片语法类似 [:]\n追加.append\n指定位置插入.insert\n追加另一个list.extend 或 list1 + list2\n三种移除方式 .pop、.remove、del list[idx]\n是否包含 x in list\nTuple 声明方式： tuple = (1, '2', xxx) 单元素声明： tuple = (1,)\n单元素声明要带逗号，否则会被认为是该单元素的数据类型\n与List不同的是Tuple是不可变的，但也不是完全不可变：\n不可变指：Tuple的每个元素，指向永远不变。但是可以改变Tuple中元素的元素（例如元素List中再追加、改变元素的属性等）\n其他相关操作除了没有修改之外与List相同\nDict 声明方式： dict = {1 : xx, 'yy' : 2}\nkey必须是不可变的：数字，字符串或元组\n相关操作 .clear 清空\n.items 以列表返回可遍历的(键, 值) 元组数组\nSet 声明方式： s = set([1, '2', xxx]) 使用List作为输入\n相关操作 添加 .add\n移除一个元素 .remove\n并集 | 交集 \u0026amp; 差集 -\n函数参数 默认参数 def print_user_info( name , age , sex = \u0026#39;男\u0026#39; ): 默认参数只能放在末尾 默认参数只能是不可变对象（字符串、数字、元组、布尔、None） 如果用可变对象做默认参数，会出现默认值被改变\n关键字参数 print_user_info(name=\u0026#39;甜味梨\u0026#39;, sex=\u0026#39;女\u0026#39;, age=18) 在调用时指定参数关键字进行赋值\n如果想要对于某些形参强制指定需要使用关键字参数，可以在定义时通过 * 分隔形参，*后的形参为需要指定关键字的\ndef print_user_info(name, *, age, sex=\u0026#39;男\u0026#39;): 不定长参数（位置参数） 通过在形参前加 * 来声明不定长参数，实际上是用一个元组接收 如果想要在调用时也能使用关键字参数，需要在形参前加 **，这样会将接收到的参数存放到字典中 例子：\ndef print_user_info(name, age, *args, sex=\u0026#39;男\u0026#39;, **kwargs): print(\u0026#39;昵称：{}\u0026#39;.format(name) , end = \u0026#39; \u0026#39;) print(\u0026#39;年龄：{}\u0026#39;.format(age) , end = \u0026#39; \u0026#39;) print(\u0026#39;性别：{}\u0026#39;.format(sex) ,end = \u0026#39; \u0026#39; ) print(\u0026#39;args:{}\u0026#39;.format(args),end = \u0026#39; \u0026#39;) print(\u0026#39;kwargs:{}\u0026#39;.format(kwargs)) if __name__ == \u0026#39;__main__\u0026#39; : print_user_info(\u0026#39;甜味梨\u0026#39;, 18, \u0026#39;arg1\u0026#39;, \u0026#39;arg2\u0026#39;, hobby=(\u0026#39;打篮球\u0026#39;,\u0026#39;打羽毛球\u0026#39;,\u0026#39;跑步\u0026#39;), height=179) # output: # 昵称：甜味梨 年龄：18 性别：男 args:(\u0026#39;arg1\u0026#39;, \u0026#39;arg2\u0026#39;) kwargs:{\u0026#39;hobby\u0026#39;: (\u0026#39;打篮球\u0026#39;, \u0026#39;打羽毛球\u0026#39;, \u0026#39;跑步\u0026#39;), \u0026#39;height\u0026#39;: 179} lambda 函数 lambda [arg1 [,arg2,.....argn]]:expression lambda函数是一个表达式而非代码块，所以拥有自己的命名空间，且不能访问自有参数列表之外或全局命名空间里的参数。 换句话说，lambda用到的参数只能通过形参传入，不能用外部的变量，否则会出错。\n坑：\nif __name__ == \u0026#39;__main__\u0026#39; : num2 = 100 sum1 = lambda num1 : num1 + num2 num2 = 10000 sum2 = lambda num1 : num1 + num2 num2 = 1 print(sum1(1)) print(sum2(1)) # output: 2 2 原因：num2变量在运行时绑定值，而不是定义时就绑定\n迭代器和生成器 迭代器 使用 iter() 生成一个迭代器，通过 next() 取下一个值，或者在for循环中使用 in 依次取值。\n只有实现了 __iter__ 方法的对象才能使用生成器，否则需要自己写。\n反向迭代需要实现 __reversed__ 方法。\n生成器 在函数声明中使用 yield 返回一个生成器对象而不是像return一样返回结果。\nyield返回后当前的这个生成器函数会记录当前状态，在下次调用时从暂停处继续。\n优势：惰性求值的特点可以节约内存，适合处理大型数据集或创建复杂的数据流。\n面向对象 实例方法和类方法 类中函数上加 @classmethod 注解表示这个方法是类方法，绑定在类上，调用时不需要实例化直接类名.方法调用。\n实例属性和类属性 实例属性在 __init__ 函数中声明，类属性在类中声明。\nps: 一个类只能有一个构造器，但是可以通过类方法变相实现多个构造器。\n访问控制 在属性或方法前加两个下划线 __ ，表明私有性，不能在类外访问且不能被继承。\n私有性实现的原理是通过名称改写，如 __var1 私有属性会被改写为 _类名称__var1，因此按照定义的名称访问不到， 但是通过改写后的名称是可以访问的。所以所谓“私有”只是方便开发所做的优化，并不是真的私有。 动态修改 基于python动态语言的特性，在类外可以通过声明一个新方法并 className.oldMethod = newMethod 对旧方法进行重写，修改会应用到所有已经创建的实例中。\n同理对于类属性的修改和增添也可以在类外动态修改。\n实例属性可以动态增添修改，但不能对实例方法进行重写。\n模块与包 导入 import moduleName 直接导入模块，调用时 moduleName.xxx\nfrom moduleName import xxx 会将该模块中的指定类或方法或属性导入当前模块，调用时就不用加 moduleName 而是直接使用导入模块中的命名。\n主模块 如果这个模块没有被引用过，那么就是主模块，否则为非主模块。有一个系统变量__name__就是用来记录模块属性的。\n包 为了避免模块名冲突，python又引入了按目录来组织模块的方法，称为包（Package）。\n对于拥有 __init__.py 文件的目录，python会将其视为一个包。\n闭包 闭包是一个函数值，它引用了其函数体之外的变量。通过某种数据结构存储一个函数和一个关联的上下文环境，实现闭包的特性。\n例子：\ntime = 0 def study_time(time): def insert_time(min): nonlocal time time = time + min return time return insert_time f = study_time(time) print(f(2)) # 2 print(time) # 0 print(f(10)) # 12 print(time) # 0 python中在函数的__closure__属性中保存上下文环境，从而使得每个闭包都能拥有独立的函数外部变量。\nps:\nnonlocal 在函数内部声明一个外部函数的变量 global 在函数内部声明一个全局变量 装饰器 装饰器利用闭包的特性实现\n作用：不修改原有函数代码的基础上，动态地增加或修改函数的功能。解耦通用的功能。\ndef my_decorator(func): def wrapper(*args, **kwargs): print(\u0026#34;Something is happening before the function is called.\u0026#34;) result = func(*args, **kwargs) print(\u0026#34;Something is happening after the function is called.\u0026#34;) return result return wrapper @my_decorator def say_hello(name): print(f\u0026#34;Hello, {name}!\u0026#34;) say_hello(\u0026#34;Alice\u0026#34;) # output: # Something is happening before the function is called. # Hello, Alice! # Something is happening after the function is called. ","permalink":"http://localhost:1313/posts/python/python/","summary":"python基础语法，包括数据类型、函数、类等","title":"python入门"},{"content":"Postgres 批量删除语句超时 场景 播放记录表history中大约有将近10亿条数据，为了减少内存占用，后台任务以每秒一次的速度删除超过1年未更新的旧记录。\n表结构（示意结构） Table \u0026#34;public.history\u0026#34; Column | Type | Collation | Nullable | Default --------+--------+-----------+----------+--------- uid | text | | not null | vid | text | | not null | ts | bigint | | not null | Indexes: \u0026#34;history_pkey\u0026#34; PRIMARY KEY, btree (uid, vid) \u0026#34;history_ts\u0026#34; btree (ts) 删除语句 delete from history where (uid, vid) in (select uid, vid from history where ts \u0026lt;= $1 limit 500) 问题出现 任务稳定运行了一年多的时间，突然有一天线上日志出现了大量的超时报错，排查后发现是pg执行删除语句的时候超过了最大时限 pq: canceling statement due to user request\n问题排查 分析语句执行计划 EXPLAIN ANALYZE statement查看该语句的执行情况，发现长时间无响应，缩小limit数量重试，得到：\nQUERY PLAN ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ Delete on history (cost=32.45..119.05 rows=83 width=87) (actual time=3791.173..3791.175 rows=0 loops=1) -\u0026gt; Nested Loop (cost=32.45..119.05 rows=83 width=87) (actual time=3774.474..3789.531 rows=10 loops=1) -\u0026gt; HashAggregate (cost=31.75..31.85 rows=10 width=138) (actual time=3771.885..3771.904 rows=10 loops=1) Group Key: \u0026#34;ANY_subquery\u0026#34;.uid, \u0026#34;ANY_subquery\u0026#34;.vid Batches: 1 Memory Usage: 24kB -\u0026gt; Subquery Scan on \u0026#34;ANY_subquery\u0026#34; (cost=0.00..31.70 rows=10 width=138) (actual time=103.109..3771.850 rows=10 loops=1) -\u0026gt; Limit (cost=0.00..31.60 rows=10 width=57) (actual time=103.104..3771.828 rows=10 loops=1) -\u0026gt; Seq Scan on history history_1 (cost=0.00..48477444.80 rows=15343146 width=57) (actual time=103.103..3771.821 rows=10 loops=1) Filter: (ts \u0026lt;= \u0026#39;1678603902000\u0026#39;::bigint) Rows Removed by Filter: 6974992 -\u0026gt; Index Scan using history_pkey on history (cost=0.70..8.72 rows=1 width=63) (actual time=1.757..1.757 rows=1 loops=10) Index Cond: ((uid = \u0026#34;ANY_subquery\u0026#34;.uid) AND (vid = \u0026#34;ANY_subquery\u0026#34;.vid)) Planning Time: 0.376 ms Execution Time: 3791.369 ms (14 行记录) 发现主要阻塞在子查询，优化器竟然选择了Seq Scan顺序扫描，大量不符合条件的数据被Filter筛掉，时间就耗在这个上边了。\n啊？ts上不是有索引么，这么大的表竟然选择了顺序扫描？\n问一下GPT:顺序和索引扫描是如何取舍的？ 在PostgreSQL中，即使在选择相关列上存在索引，查询计划也可能选择顺序扫描（Sequential Scan）而不是索引扫描（Index Scan）。这是因为查询优化器需要基于多种因素做出决策，如预计的查询代价，表大小，数据的分布等。以下是一些可能的原因：\n全表扫描优于索引扫描：倘若你要查询的数据占据了表的大部分，顺序扫描可能更有效。PostgreSQL查询优化器会权衡全表扫描和索引扫描的代价，如果全表扫描的预计代价更低，那么它可能会选择全表扫描。\n统计信息不准确：PostgreSQL的查询优化器基于统计信息对查询计划进行优化。如果表的统计信息不准确（例如，表数据已经发生了大幅度变化但是没有重新收集统计信息），查询优化器可能会做出错误的决策，即使索引扫描更高效，也可能选择顺序扫描。\n索引列的选择性不高：如果索引列的值重复度很高（例如，性别、是否启用等值只有几种的列），那么使用索引扫描的效率可能并不高，查询优化器可能会选择顺序扫描。\n系统参数设置的问题：某些PostgreSQL系统参数可能影响到索引的使用，比如enable_seqscan参数控制了是否优先使用顺序扫描。\n如果你注意到了查询计划中一直使用序列扫描而不是索引扫描，且索引扫描明显更高效，你可能需要重新收集统计信息（使用ANALYZE命令），或者检查你的查询是否能够有效的利用索引，或者审查并调整相关的系统参数。\n逐一排查 全表扫描优于索引扫描？不确定。全表10亿条数据，全表扫描会比索引扫描更高效么？\n统计信息不准确？不确定。\n索引列的选择性不高？不太可能🙅。ts是按照观看时间排序的，不存在重复度很高的情况。\n系统参数设置的问题？不太可能🙅。任务正常运行一段时间了。\nps：EXPLAIN中cost代表查询优化器估算的查询代价，原因一关于全表扫描和索引扫描的取舍就取决于优化器对两种扫描方式的代价估算，而估算则需要参考原因二提到的统计信息，也就是说一二是相关的。\n有索引但不走的情况有哪些？ 表很小（100行以内），顺序扫描的效率可能会高于索引扫描。\n返回结果占总表很大比例。\n当LIMIT数量很小时，优化器可能会认为“顺序扫描直到结果达到限制数量后终止”会更快的找到结果；但这是双刃剑，依赖于表统计数据的准确性。\n优化器参考的统计信息有哪些？ 表的大小：一般来说，对于非常小的表，顺序扫描往往更有效，因为索引扫描涉及到查找索引并获取磁盘上数据的过程，这对于小表来说可能比直接顺序扫描更耗时。相反，对于大表，索引扫描往往能显著提高效率。\n索引的选择性：索引的选择性是指通过索引能够区分的记录的百分比。如果索引的选择性较高（例如，主键索引），邐那么索引扫描往往更有效。\n查询的选择度：如果查询过滤大部分数据，那么索引扫描对于性能的提升更明显。因为顺序扫描需要扫描整个表，而索引扫描只需要访问满足条件的索引项和相关记录。\n硬件性能：硬件的 IO 性能和 CPU 性能也会影响扫描方式的选择。IO 密集的工作负载可能更适合使用索引扫描，而 CPU 密集的工作负载可能更适合使用顺序扫描。\n索引和表的物理顺序：如果表的物理顺序与索引的顺序高度相关（correlation 接近 1），那么使用索引扫描的成本可能降低，因为顺序 IO 往往比随机 IO 更高效。\n问题定位 通过排查可以猜测：是优化器在执行查询命令时，对代价估算出现了错误，导致选择了实际性能更差的全表扫描方法。\n横向对比其他表 除了history表之外，数据库中还有两个结构相似、数据量相似、索引设置相似的记录表history_a,history_b（简称a表b表），其中a表和history一样在执行后台删除任务，b表没有执行过删除任务。\n浅看一眼表内总行数：\nx=\u0026gt; SELECT relname, reltuples FROM pg_class r JOIN pg_namespace n ON (relnamespace = n.oid) WHERE relkind = \u0026#39;r\u0026#39; AND n.nspname = \u0026#39;public\u0026#39;; relname | reltuples -------------------+--------------- history_a | 7.233728e+08 history | 9.3278854e+08 history_b | 8.412001e+08 对history、a、b分别执行查询命令：\nx=\u0026gt; explain analyze select uid, vid from history_a where ts \u0026lt;= 1678692987000 limit 100; QUERY PLAN ---------------------------------------------------------------------------------------------------------------------------------------------------------- Limit (cost=0.57..329.24 rows=100 width=57) (actual time=0.074..11.583 rows=100 loops=1) -\u0026gt; Index Scan using history_a_ts_1 on history_a (cost=0.57..49428562.93 rows=15039219 width=57) (actual time=0.073..11.573 rows=100 loops=1) Index Cond: (ts \u0026lt;= \u0026#39;1678692987000\u0026#39;::bigint) Planning Time: 1.400 ms Execution Time: 11.617 ms (5 行记录) x=\u0026gt; explain analyze select uid, vid from history where ts \u0026lt;= 1678692987000 limit 100; QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------------------- Limit (cost=0.00..262.93 rows=100 width=57) (actual time=519.878..14138.338 rows=100 loops=1) -\u0026gt; Seq Scan on history (cost=0.00..48477444.80 rows=18437250 width=57) (actual time=519.877..14138.275 rows=100 loops=1) Filter: (ts \u0026lt;= \u0026#39;1678692987000\u0026#39;::bigint) Rows Removed by Filter: 24919480 Planning Time: 0.671 ms Execution Time: 14138.486 ms (6 行记录) x=\u0026gt; explain analyze select uid, vid from history_b where ts \u0026lt;= 1678692987000 limit 100; QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------------------------- Limit (cost=0.00..14.53 rows=100 width=77) (actual time=2.032..12.098 rows=100 loops=1) -\u0026gt; Seq Scan on history_b (cost=0.00..42239628.56 rows=290642848 width=77) (actual time=2.031..12.085 rows=100 loops=1) Filter: (ts \u0026lt;= \u0026#39;1678692987000\u0026#39;::bigint) Rows Removed by Filter: 1033 Planning Time: 0.610 ms Execution Time: 15.866 ms (6 行记录) 好奇怪，发现a表选择了索引扫描，b和history表都是顺序扫描，但是b表执行速度显著快于history表。 纵向对比自己 x=\u0026gt; explain analyze select uid, vid from history where ts \u0026lt;= 1678763273000 limit 500; QUERY PLAN --------------------------------------------------------------------------------------------------------------------------------------- Limit (cost=0.00..1166.31 rows=500 width=57) (actual time=10.661..7664.132 rows=500 loops=1) -\u0026gt; Seq Scan on history (cost=0.00..48421139.20 rows=20758242 width=57) (actual time=10.660..7663.903 rows=500 loops=1) Filter: (ts \u0026lt;= \u0026#39;1678763273000\u0026#39;::bigint) Rows Removed by Filter: 14947898 Planning Time: 0.056 ms Execution Time: 7666.074 ms (6 行记录) x=\u0026gt; explain analyze select uid, vid from history where ts \u0026lt;= 1678763273000 order by ts limit 500; QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------------------------------------------- Limit (cost=0.57..1595.46 rows=500 width=65) (actual time=1.700..64.891 rows=500 loops=1) -\u0026gt; Index Scan using history_ts_1 on history (cost=0.57..66214154.35 rows=20758242 width=65) (actual time=1.699..64.824 rows=500 loops=1) Index Cond: (ts \u0026lt;= \u0026#39;1678763273000\u0026#39;::bigint) Planning Time: 0.053 ms Execution Time: 64.949 ms (5 行记录) 添加了order by强制命令走索引，发现走索引的估算cost比顺序扫描的估算cost要低，说明了优化器为什么会选择顺序扫描。同样也说明估算出现了问题。 猜想 批量删除导致了顺序扫描耗时增加，可能和频繁删除插入导致磁盘物理顺序乱序有关，因此在遍历时需要访问更多的内存页。 优化器在history表上决策执行计划时参考的统计数据有问题，导致代价估算出现偏差，可能的原因是没有及时更新统计数据。 验证 查询history表和a表的状态信息表，查看最后执行vacuum和analyze的时间。\nx=\u0026gt; select relname,last_analyze,last_autoanalyze,last_autovacuum from pg_stat_user_tables where relname=\u0026#39;history_a\u0026#39; or relname=\u0026#39;history\u0026#39;; -[ RECORD 1 ]----+------------------------------ relname | history_a last_analyze | last_autoanalyze | 2024-03-13 07:05:50.585392+00 last_autovacuum | 2024-03-11 13:13:25.05856+00 -[ RECORD 2 ]----+------------------------------ relname | history last_analyze | last_autoanalyze | last_autovacuum | 2024-03-13 16:54:04.673357+00 history表竟然没有配置auto analyze。。。。。。（b表也没配） 问题解决 手动执行 ANALYZE analyze history会以采样的方式对表统计数据进行估算，所以速度相对较快；并且只会阻塞如 vacuum、创建索引、reindex、alter table等命令，不会影响表的读写。\n重新执行命令 x=\u0026gt; explain analyze select uid, vid from history where ts \u0026lt;= 1678795619000 limit 500; QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------------------------------------------- Limit (cost=0.57..1978.10 rows=500 width=57) (actual time=11.670..16.685 rows=500 loops=1) -\u0026gt; Index Scan using history_ts_1 on history (cost=0.57..495456.97 rows=125272 width=57) (actual time=11.669..16.638 rows=500 loops=1) Index Cond: (ts \u0026lt;= \u0026#39;1678795619000\u0026#39;::bigint) Planning Time: 11.769 ms Execution Time: 17.972 ms (5 行记录) 问题解决。\n此外，还可以修改limit值观察一下是不是当limit很小时会顺序查询：\nx=\u0026gt; explain analyze select uid, vid from history where ts \u0026lt;= 1710418557000 limit 1; QUERY PLAN ---------------------------------------------------------------------------------------------------------------------------------- Limit (cost=0.00..0.05 rows=1 width=57) (actual time=1.055..1.056 rows=1 loops=1) -\u0026gt; Seq Scan on history (cost=0.00..48494915.20 rows=924846115 width=57) (actual time=1.054..1.054 rows=1 loops=1) Filter: (ts \u0026lt;= \u0026#39;1710418557000\u0026#39;::bigint) Planning Time: 6.117 ms Execution Time: 1.076 ms (5 行记录) 果然如此。\n索引优化 在寻找问题的过程中，发现索引内存占用较大，可能与数据频繁的删除新增有关，称之为索引膨胀\n查询数据库中所有表的大小：\nSELECT table_name, pg_size_pretty(table_size) AS table_size, pg_size_pretty(indexes_size) AS indexes_size, pg_size_pretty(total_size) AS total_size FROM ( SELECT table_name, pg_table_size(table_name) AS table_size, pg_indexes_size(table_name) AS indexes_size, pg_total_relation_size(table_name) AS total_size FROM ( SELECT (\u0026#39;\u0026#34;\u0026#39; || table_schema || \u0026#39;\u0026#34;.\u0026#34;\u0026#39; || table_name || \u0026#39;\u0026#34;\u0026#39;) AS table_name FROM information_schema.tables where table_schema = \u0026#39;public\u0026#39; ) AS all_tables ORDER BY total_size DESC ) AS pretty_sizes; reindex:\nREINDEX TABLE CONCURRENTLY history; 或者手动\nCREATE INDEX CONCURRENTLY history_ts_1 ON history(ts); DROP INDEX CONCURRENTLY history_ts; CREATE UNIQUE INDEX CONCURRENTLY history_pkey_new ON history(uid, vid); ALTER TABLE history DROP CONSTRAINT history_pkey, ADD CONSTRAINT history_pkey PRIMARY KEY USING INDEX history_pkey_new; ","permalink":"http://localhost:1313/posts/postgres/pg_timeout/","summary":"\u003ch1 id=\"postgres-批量删除语句超时\"\u003ePostgres 批量删除语句超时\u003c/h1\u003e\n\u003ch2 id=\"场景\"\u003e场景\u003c/h2\u003e\n\u003cp\u003e播放记录表\u003ccode\u003ehistory\u003c/code\u003e中大约有将近10亿条数据，为了减少内存占用，后台任务以每秒一次的速度删除超过1年未更新的旧记录。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e表结构（示意结构）\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e\u003cspan class=\"k\"\u003eTable\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;public.history\u0026#34;\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003eColumn\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"k\"\u003eType\u003c/span\u003e\u003cspan class=\"w\"\u003e  \u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003eCollation\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003eNullable\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003eDefault\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"c1\"\u003e--------+--------+-----------+----------+---------\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003euid\u003c/span\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nb\"\u003etext\u003c/span\u003e\u003cspan class=\"w\"\u003e   \u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"w\"\u003e           \u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enot\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enull\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003evid\u003c/span\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nb\"\u003etext\u003c/span\u003e\u003cspan class=\"w\"\u003e   \u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"w\"\u003e           \u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enot\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enull\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ets\u003c/span\u003e\u003cspan class=\"w\"\u003e     \u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"nb\"\u003ebigint\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"w\"\u003e           \u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enot\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003enull\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"o\"\u003e|\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e\u003c/span\u003e\u003cspan class=\"n\"\u003eIndexes\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;history_pkey\u0026#34;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003ePRIMARY\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"k\"\u003eKEY\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ebtree\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003euid\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003evid\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;history_ts\u0026#34;\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"n\"\u003ebtree\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ets\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003e删除语句\n\u003ccode\u003edelete from history where (uid, vid) in (select uid, vid from history where ts \u0026lt;= $1 limit 500)\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"问题出现\"\u003e问题出现\u003c/h2\u003e\n\u003cp\u003e任务稳定运行了一年多的时间，突然有一天线上日志出现了大量的超时报错，排查后发现是pg执行删除语句的时候超过了最大时限 \u003ccode\u003epq: canceling statement due to user request\u003c/code\u003e\u003c/p\u003e","title":"Postgres 批量删除语句超时"},{"content":"哨兵机制 参考 Redis 核心技术与实战\n小林coding redis\n原理 引入哨兵的目的 主从复制机制可以保证从库断连恢复后数据一致性以及整体服务的可用性，但是如果主库故障就会导致写操作阻塞以及同步终止，这时就需要引入哨兵机制了（Sentinel）。\n哨兵的三任务 哨兵也是一个独立的redis进程，负责三个任务：监控、选主、通知\n监控：周期性地给所有的主从库发送 PING 命令，检测它们是否仍然在线运行。如果响应超时就会把对应节点标记为“下线状态”，如果主库下线，就需要进入选主任务 选主：当主库挂了就需要根据一定的规则（后边会说）选择一个从库实例作为新主库 通知：将新主库连接信息同步给其他从库和客户端，让主从重新建立起同步连接 哨兵集群 单哨兵的问题：\n单个哨兵故障会导致无法进行主从切换 哨兵自身网络延迟导致误判主节点下线 解决：哨兵集群\n原理：发布者/订阅者机制\n注意事项：所有哨兵的配置应该保持一致\n哨兵集群配置 哨兵配置文件中： sentinel monitor \u0026lt;master-name\u0026gt; \u0026lt;ip\u0026gt; \u0026lt;redis-port\u0026gt; \u0026lt;quorum\u0026gt; 可见哨兵只需要知道主节点ip端口，那么哨兵之间以及哨兵从节点之间是怎么感知的呢？\n哨兵之间如何建立连接 依托发布订阅机制。主库中有一个sentinel:hello”频道，当一个新哨兵加入后，会向所有订阅该频道的哨兵广播新哨兵的ip端口号，从而建立连接。\n哨兵从节点之前如何建立连接 哨兵向主库发送INFO命令获取所有从库ip端口，从而建立连接。\n监控 主观下线、客观下线 当哨兵发现响应超时后，会判断主库为“主观下线”状态，即“我认为主库下线了”，然后发送命令询问其他哨兵是否赞同这个判断，当赞同票超过配置文件中的quorum数量时，就可以标记主库为“客观下线”， quorum值默认是n/2+1（n为哨兵数）。\nps：客观下线只针对主库，对于从库判定主观下线就够了\n选主 在正式开始之前需要确认一点：选主的任务是由哪个哨兵执行的？\n投票选取leader哨兵 从候选者中通过投票选出leader负责接下来的主从故障转移。每个哨兵只有一次投票机会，候选者会把票投给自己。\n候选者是刚刚判定主节点客观下线的所有哨兵。\n成为leader条件：\n拿到半数以上赞成票 大于等于哨兵配置文件中的quorum值 选择新的主节点 选择的原则：（筛选+打分）打分阶段遇到最高分从库就选择它\n留下当前在线且之前超时次数小于10次的从库 优先级最高的从库得分高，优先级由用户配置 和旧主库同步程度最接近的从库得分高，主库从库在环形缓存数组（repl_backlog_buffer）上都有自己的复制进度offset，判断主从offset的距离 ID 号小的从库得分高，ID是实例编号 通知 通知从库和客户端\n通知从库 首先，向选定的节点发送slaveof no one命令将其升级为新主节点，然后通过slaveof ip port将其他所有从节点连接到新主节点。\n此外，还需要监听旧主节点，当他重新上线时需要将其转变成新主节点的从库。\n通知客户端 主从故障转移的每个事件都对应发布订阅机制中的一个频道，客户端可以订阅对应频道的消息得知主从切换状态。\n总结 多节点之前怎么协调的：\n相互感知：发布订阅 做决定：投票 ","permalink":"http://localhost:1313/posts/redis/sentinel/","summary":"Redis的哨兵机制解决了主库故障的问题，通过监控、选主和通知实现高可用性和故障转移，提供可靠的服务。","title":"redis 哨兵机制"},{"content":"质数筛法 问题：判断1到n中有哪些质数（LC.204 质数计数）\n算术基本定理 算术基本定理，又称为正整数的唯一分解定理，即：每个大于1的自然数，要么本身就是质数，要么可以写为2个或以上的质数的积，而且这些质因子按大小排列之后，写法仅有一种方式。\n推论：任何一个合数都可以表示成一个质数和另一个数的乘积\n埃氏筛 sieve of Eratosthenes 埃氏筛（ sieve of Eratosthenes）对于每一个质数，标记它在n以内的所有倍数，根据推论可知当遍历结束后，所有合数都被标记。\n优点：相比于对n以内每个数朴素的单独判断，时间复杂度更低 缺点：对于同一个合数存在重复标记的现象（18: 2*9 3*6） 代码：\nfunc sieveOfEratosthenes(n int) { notPrime := map[int]bool{1: true} // 取反可以省略初始化过程 for i := 2; i*i \u0026lt;= n; i++ { if !notPrime[i] { for j := i * i; j \u0026lt;= n; j += i { // j从i*i开始，因为小于i*i的值已经在之前判断过了 notPrime[j] = true } } } } 欧拉筛 sieve of Euler 欧拉筛对于1到n的每个数，标记它与比它小的所有质数分别相乘得到的合数，在标记时注意保证合数是由其最小的质因数得到的（保证唯一标记的原理）。根据推论可知当遍历结束后，所有合数都被标记。\n优点：比埃氏筛更快，因为避免了合数的重复标记 缺点：需要额外的空间保存已经验证的质数，以空间换时间 代码：\nfunc sieveOfEuler(n int) { primes := []int{} notPrime := map[int]bool{1: true} for i := 2; i \u0026lt;= n; i++ { if !notPrime[i] { primes = append(primes, i) } for j := 0; j \u0026lt; len(primes) \u0026amp;\u0026amp; i*primes[j] \u0026lt;= n; j++ { notPrime[i*primes[j]] = true if i%primes[j] == 0 { // 保证每个合数都被其最小质因子筛去 break } } } } 代码中 i%primes[j] == 0 什么意思 假设：i = 4 时 primes = []int{2, 3}\n如果不做i%primes[j] == 0判断，那么会标记合数 4*3=12，\n那么当i = 6时，就会出现合数6*2=12的重复标记，而欧式筛正是为了避免这种情况的出现，对于合数，只由他的最小质因数筛去。\n为什么primes[j]之后的质数都跳过了？ 因为i是primes[j]的整数倍，那么他再乘后续的每个数都相当于(a*primes[j])*x（a：倍数，x：后续某个数），\n等价于primes[j]*(a*x)，这个a*x就留给之后的i了，就是为了保证每个合数只由他的最小质因数筛去。\n","permalink":"http://localhost:1313/posts/algorithm/prime/","summary":"文章介绍了判断质数的两种筛法：埃氏筛标记质数倍数但会重复标记合数，欧拉筛避免了重复但需额外空间存储验证过的质数。","title":"质数筛法"},{"content":"主从复制 参考 Redis 核心技术与实战\n小林coding redis\n原理 启用多台服务器，使用一主多从的结构，将数据备份到每一台服务器上。\n目的：分摊主服务器压力、提高可用性\n主从复制共有三种模式：全量复制、基于长连接的命令传播、增量复制。\n单点模式面临的问题： 所有请求都由一台机器处理，压力过大 服务器宕机导致服务不可用 主从复制模式优势： 读写分离。主库处理写请求，从库负责读。 一台服务器宕机不会影响整体服务 主从复制模式问题： 数据一致性问题（同步机制） 需要考虑故障转移（哨兵机制） 全量复制（初次同步） 初次同步三阶段：建立连接、同步数据、查漏补缺 建立连接 命令： replicaof 192.168.1.1 6379\n执行该命令将当前实例变成指定ip:host实例的从库。\n网络层面看，当前实例给主服务器发送psync同步请求（psync ? -1 ?表示未知的主库runID，-1表示第一次复制），主服务器收到后会返回FULLRESYNC全量复制命令响应，同时会携带runID（redis实例id）和offset（复制进度）\n连接建立以后，就准备进入全量复制阶段了。\n同步数据 主库通过bgsave命令通过fork出的子进程异步生成RDB文件，然后将文件发给从库。从库收到后，先清空当前数据库，然后加载RDB文件，为了避免从库中之前数据的影响。\n主从数据一致性问题 存在三个时间段，会导致新收到的写命令不会同步给从库：\n主库生成RDB文件期间 主库发送RDB文件期间 从库加载RDB文件期间 如果不进行特殊处理，数据库中数据越多RDB文件越大，导致的不一致情况也会越严重。\n解决办法：使用 replication buffer 缓冲区 在RDB发送过程中，将所有新增到的写命令记录到该缓冲区中，在收到从库的全量复制完成信号后，就进入第三阶段，复制缓冲区中的命令。\n查漏补缺 主库将replication buffer中的所有记录发送给从库，至此，主从之间的第一次同步就完成了。\n基于长连接的命令传播 在主从库完成第一次同步后，双方之间维护一个TCP长连接，目的是避免频繁的断开和连接带来的性能开销。\n当主库收到写操作时，会通过长连接传播给从库。\n增量复制 当主从库之间出现了网络问题，如何同步网络断连期间主库新增的写命令？\n重新进行全量复制（redis 2.8以前），开销很大 增量复制（目前采用的方法） 网络恢复后如何确定断开期间主库增量的数据？repl_backlog_buffer环形缓冲区。\nrepl_backlog_buffer repl_backlog_buffer是一个大小可指定的环形缓冲区，写操作不仅会写入 replication buffer，还会写入该环形缓冲区。主库和从库分别维护自己写到或读到的位置。\n当主从库之间的连接重新建立后，从库会发送psync命令请求同步，其中会携带复制进度offset。主库从而可以根据自己的offset和从库的offset判断断线期间新增的写命令有哪些，然后依次同步给从库。\n如果主库判断offset时发现从库要读取的数据已经被覆盖了，那么就会采用全量同步。\nps：环形缓冲区写满后，继续写入会覆盖掉之前的数据。如果从库的读取速度比较慢，就有可能导致从库还未读取的操作被主库新写的操作覆盖了，这会导致主从库间的数据不一致。\n解决：调整缓冲区大小\n总结 主从复制三阶段、主从网络延迟的数据一致性保证\n问题：主库故障或断连，如何保证服务可靠性？\n解决：哨兵集群\n","permalink":"http://localhost:1313/posts/redis/replication/","summary":"Redis主从复制将数据备份到多个从服务器，提高可用性。主从同步包括全量复制、基于长连接的命令传播、增量复制阶段。","title":"redis 主从复制机制"},{"content":"持久化之rdb和混合快照 参考 Redis 核心技术与实战\n小林coding redis\n定义 RDB：Redis DataBase\nRDB 记录的是某一时刻的数据，因此恢复效率会比aof方式高。\n全量快照 记录某一时刻的全部数据\n同步执行 通过save命令在主进程中生成rdb文件，写入期间会阻塞主进程。\n问题：主进程可能长时间阻塞\n异步执行 通过bgsave命令或者在配置文件中配置，由创建的子进程生成rdb文件，可以避免主进程阻塞。\n同时，使用写时复制技术保证了快照的完整性，也允许主线程同时对数据进行修改，避免了对正常业务的影响。但是，也导致主进程对数据的修改不会应用在本次的快照之中。\n问题：\n执行频率过高：频繁将全量数据写入磁盘，造成磁盘压力过大；另外，频繁fork子进程也会阻塞主进程。 执行频率过低：宕机会导致两次快照期间的数据丢失。 增量快照 做一次全量快照，后续每个快照时刻只需要记录增量修改的元数据信息。\n问题：为了记录修改信息引入的额外空间开销较大。\n混合快照 通过配置文件开启混合快照，使用rdb格式记录全量数据，同时使用aof格式记录两次快照间的增量记录，在下一次快照时清空aof记录。\n优势：兼顾了rdb恢复速度快和aof数据少丢失的优点\n","permalink":"http://localhost:1313/posts/redis/rdb/","summary":"RDB记录了某一时刻的数据，恢复效率高。全量快照可以同步或异步执行，异步执行使用写时复制技术，避免了主进程阻塞。增量快照记录了增量修改的元数据信息。混合快照通过同时使用RDB和AOF格式，既提供快速恢复又减少数据丢失。","title":"redis 持久化之RDB"},{"content":"持久化之AOF日志 参考 参考 Redis 核心技术与实战\n小林coding redis\n定义 AOF：Append Only File 保存写命令到日志，当数据需要恢复时执行aof文件中记录的所有写操作命令。\n特点 写后日志 Redis先执行命令把数据写入内存，然后再记录日志\n优点：\n避免额外的检查开销 不会阻塞当前写命令的执行 缺点\n数据可能因为宕机丢失 可能阻塞下一个命令 文本形式保存 命令set k v在aof文件中的表现形式：\n*3 --\u0026gt; 单词数 $3 --\u0026gt; 字符数 set $1 k $1 v 优点：\n可读性强，方便调试 容错性更强，一部分数据损坏不会影响整个文件的解析 缺点：\n空间占用较大 存在冗余记录（需要重写） 写回策略 写回：写入磁盘\n三种写回策略 在配置文件中appendfsync可填写三种参数：Always, Everysec, No， 三种策略是对于主进程阻塞和减少数据丢失两个问题的取舍。\naof写入流程 执行写命令 -\u0026gt; 写到用户态aof缓冲区 -\u0026gt; 发起I/O系统调用write()将aof缓冲区数据拷贝到内核缓冲区page cache -\u0026gt; 排入队列，由内核决定何时写入磁盘\n由此可见，性能瓶颈集中在write系统调用将数据拷贝到系统缓冲区步骤上，在此期间主进程会被阻塞。\n操作系统的fsync()系统调用可以立即将内核缓冲区中的数据写入磁盘，三种写入策略对应着三种fsync的调用时机：\nAlways 策略就是每次写入 AOF 文件数据后，就执行 fsync() 函数； Everysec 策略就会创建一个异步任务来执行 fsync() 函数； No 策略就是永不执行 fsync() 函数; ps：为什么不使用多线程？ 因为多线程需要涉及共享资源的并发控制，实现复杂且会有性能损耗。\n重写机制 当aof文件过大时（默认64MB），会从主进程fork出一个子进程，在后台进行aof的重写。\n重写过程：\n子进程扫描数据库中（内存Redis数据）所有数据，逐一把内存数据的键值对转换成一条命令，再将命令记录到重写日志。 向主进程发送一条信号表示转换完成，主进程收到信号后通过信号处理函数将aof重写缓冲区中的内容追加到新的aof文件中。 主进程对新的aof文件进行改名，覆盖现有的aof文件。 fork fork出子进程而不是子线程\n多线程间会共享内存，当修改共享内存中的数据时就需要加锁来保证数据安全，造成性能的降低。\n而子进程会复制主进程的页表（记录着虚拟地址和物理地址映射关系），而不是物理内存中的所有数据，并且子进程对这个共享的内存拥有只读的权限，当父子进程任意一方进行修改时，就会发生写时复制。\n目的：提升性能，减少fork过程阻塞时间 写时复制 在发生写操作的时候，操作系统才会去复制物理内存中要被修改的这块数据，生成该数据的副本。然后在这个数据副本上进行修改。\n目的：节约物理内存资源 重写期间有数据被修改了，导致父子进程redis数据不一致的问题 解决办法：启用aof重写缓冲区\nredis内存中设置了重写缓冲区，当子进程创建以后开始使用。\n当主进程执行完一个写命令之后，会同时将这个写命令写入到AOF缓冲区和AOF重写缓冲区。\n双写的目的：保证重写失败依旧拥有完整aof日志，保证重写期间的修改操作不会遗漏。 总结 aof写回 -\u0026gt; 主进程 aof重写 -\u0026gt; 子进程\n阻塞时机：\nfork子进程时，取决于页表大小 发生写时复制，取决于修改的数据对应的物理内存的大小 追加重写缓冲区中的数据时，取决于数据大小 文件重命名，覆盖现有aof文件时 缺点：\n恢复时需要重新执行所有记录，速度较慢 ","permalink":"http://localhost:1313/posts/redis/aof/","summary":"AOF是Redis的一种持久化方式，将写命令记录到文件中，用于数据恢复。它以文本形式保存，可读性强，但空间占用较大。AOF有三种写回策略，影响数据丢失和主进程阻塞。Redis通过重写机制优化AOF文件，提高性能。 AOF恢复速度较慢，但可靠。","title":"redis 持久化之AOF"},{"content":"redis数据结构 参考 pdai Redis进阶\nRedis 源码剖析与实战\n小林coding redis\nHyperLogLog算法原理\n底层结构 redisObject redisObject为不同类型的数据提供了统一的接口，使Redis可以用一致的方式处理不同类型的数据。\nstruct redisObject { unsigned type:4; // 数据类型 unsigned encoding:4; // 编码方式 // LRU 记录最末一次访问时间; 或者 LFU（8位频率，16位访问时间） unsigned lru:LRU_BITS; int refcount; // 引用计数，用于垃圾回收 void *ptr; // 指向底层数据结构实例 }; 简单动态字符串 SDS redis自己封装的字符串类型，四个字段（len现有长度，alloc分配的空间长度，flags SDS的类型，buf[]字符数组）\n与 C 语言原生字符串相比的优势 alloc和len字段\n避免了C语言中获取字符串长度需要遍历的缺点 可以保存二进制数据（C语言遇到\\0就表示字符串结束了） 字符串拼接时可以判断内存空间是否满足需求，避免溢出 字符串缩短时可以惰性释放空间，提高性能 flags字段\nflags类型是针对不同大小的字符串选择了不同比特位的int型变量，节约了内存空间。 PS：以上也是 golang 中string的设计思想（go中string拼接是开辟新内存然后依次copy过去）\n编译指令 在分配内存时采用**紧凑(packed)**的方式，避免内存对齐，同样为了节约内存。\nziplist 压缩列表 以动态数组的紧凑布局保存int或者string。在7.0被废弃。\n整体结构：\u0026lt;zlbytes\u0026gt; \u0026lt;zltail\u0026gt; \u0026lt;zllen\u0026gt; \u0026lt;entry\u0026gt; \u0026lt;entry\u0026gt; ... \u0026lt;entry\u0026gt; \u0026lt;zlend\u0026gt;\nzlbytes：列表内存占用大小 zltail：尾节点内存偏移量 zllen：节点总数 zlend：0xFF，末尾标记 entry结构：\u0026lt;prevlen\u0026gt; \u0026lt;encoding\u0026gt; \u0026lt;entry-data\u0026gt;\nprevlen：前一个entry大小 encoding：data存储的元素类型和长度 entry-data：实际数据 遍历 从左向右：计算prevlen占用内存，进而得到encoding起始地址，根据encoding编码可得数据大小，从而找到下一个entry起始地址。\n从右向左：通过prevlen和尾节点地址可以依次推导出上一个节点的地址\n优势 相对于普通数组，不需要预分配内存，且每个节点大小不必相同（通过encoding细化存储大小），节约内存。\n缺点 每次写操作（扩容缩容）都会进行内存分配 因为 encoding 对于不同类型不同大小的数据进行了细化，当数据大小变化时（新增节点导致prevlen内存占用变化），可能会引发连锁更新 元素越多对于列表中间节点的查找时间复杂度越高 listpack 紧凑列表 listpack是redis5.0中针对ziplist连锁更新的问题进行改良而来的数据结构，因此同样也属于紧凑布局。\n基本结构和ziplist相似\n整体结构：\u0026lt;lpbytes\u0026gt; \u0026lt;lplen\u0026gt; \u0026lt;entry\u0026gt; \u0026lt;entry\u0026gt; ... \u0026lt;entry\u0026gt; \u0026lt;lpend\u0026gt;\nentry结构：\u0026lt;encoding\u0026gt; \u0026lt;entry-data\u0026gt; \u0026lt;entry-len\u0026gt;\n不同：整体结构中移除了尾节点偏移量，entry结构中改为保存当前entry中 encoding+data 的总长度，并且为了从右向左遍历使用了大端存储模式并将entry-len放到entry末尾。 遍历 从左向右：解析encoding可以得到元素类型和长度，并且可以进一步计算得到len本身的长度，从而找到下一个entry起始地址。\n从右向左：通过lpbytes总长度直接定位到尾部结束标记，然后从右到左逐个字节读取entry-len（当读到的字节最高位为0表示该结束了）,获得entry前两部分大小，从而可以定位到前一项entry的末尾，如此往复。\nquicklist 快表 redis7.0之前通过双向链表+ziplist实现，7.0之后改为双向链表+listpack。\nquicklist每个节点都保存一个ziplist/listpack，在添加元素时会检查插入位置的压缩列表是否能容纳该元素，如果能容纳就直接保存到 quicklistNode 结构里的压缩列表，如果不能容纳，才会新建一个新的 quicklistNode 结构。具体一个列表中可以容纳几个元素可以在配置文件中设置。\n优势 综合了链表和数组的优势，链表：可以动态增减节点；数组：内存连续节省空间 避免列表中节点过多导致的查询时间复杂度变高，分散列表负载 可以根据性能动态调整（修改配置文件），时间换空间或者空间换时间 缺点 由于融合了两种数据结构所以维护成本更高，7.0之前使用ziplist结构无法避免连锁更新问题。\n哈希表 桶数组+拉链法。插入一对k-v数据时，计算key的hash值并找到对应桶索引，遇到冲突则新建一个entry连接到链表尾。\nentry是链表中的节点，每个节点保存着key、value（存值或者指针）以及指向下一个节点的指针。\n一个hash结构有两个hash表，一个用来存储kv，另一个为空，在扩容时使用。\n哈希表的扩容 触发条件：\n服务器目前没有执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且负载因子大于等于1。 服务器目前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且负载因子大于等于5。 ps：负载因子 = 哈希表已保存节点数量 / 哈希表大小 渐进式rehash过程： 为了避免大量数据集中拷贝带来的性能影响，故使用渐进式rehash，具体过程为：\n为那个空的哈希表分配两倍的内存空间 在rehash期间，增删改查操作都会在执行对应操作以外，按索引顺序一个一个将索引位置的数据从旧哈希表迁移到新表 当全部迁移完了，释放旧表的内存空间 和golang的map扩容思想非常相似，不过go中一个桶容量有限制，多出的放溢出桶，所以go中还存在等量扩容的情况 intset 整数集 三个字段：编码方式encoding、元素数量length、保存元素的数组contents[]。整数集实际上就是一个递增int数组。\n在添加每个元素时，intset会始终保持有序，目的是便于查找。\n其中contetns数组的int类型（int16、int32、int64）会根据元素的大小确定。\n适用于只存储int值且存储数量较少的情况。\n升级 当加入的新元素类型比现有的元素类型大时，需要对数组进行扩容：\n计算类型升级后数组所需要的空间，追加分配到原数组末尾。 从右向左依次转换原数组中的元素 添加新元素到数组末尾 优势：根据元素大小动态选择数组类型，可以节约内存资源。 ps：整数集没有降级操作。\nskiplist 跳表 跳表是一种多层有序链表（多级索引），目的是加速对于有序数据的快速定位和范围查询，单个查询平均时间复杂度 O(logN)。\ntypedef struct zskiplistNode { sds ele; // 元素值 double score; // 元素权重 struct zskiplistNode *backward; // 后向指针 struct zskiplistLevel { struct zskiplistNode *forward; // 前向指针 unsigned int span; // 跨度（离forward node的距离） } level[]; } zskiplistNode; typedef struct zskiplist { struct zskiplistNode *header, *tail; unsigned long length; int level; } zskiplist; level数组保存了当前节点在每一层的下一个节点以及跨度（如果有），最底层 level0 是一个双向链表保存所有的节点，其他的层都相当于是索引。\n问：在新建节点时如何确定在哪层？ 答：随机的。如果将每层节点数设置为下一层的一半会导致每次节点增删都可能需要调整level信息，带来额外的开销。\nradix tree 基数树 radix tree是前缀树（trie tree）的一种，相比于哈希表能更好的节省内存空间，于redis5.0引入。\nradix tree包含两类节点，非压缩节点（单个字符）和压缩节点（多字符），非叶子节点无法同时指向这两种子节点。\n叶子节点不包含字符，用于指向实际的值。\n目的：快速匹配，范围查询，节约内存\n应用：gin框架中的路由、golang中依靠radix tree对管理内存页的bitmap快速搜索\n五种基本类型 String 底层数据结构：int或SDS\n常用命令：SET GET SETNX SETEX MSET MGET INCR INCRBY EXPIRE DEL STRLEN\n应用场景：缓存对象（json作值）、计数、分布式锁、session\nList 底层数据结构：\n3.2之前：元素个数和元素大小小于阈值时使用压缩列表ziplist，否则使用双向链表 3.2之后：quicklist 常用命令：LPUSH LPOP LRANGE LINDEX BLPOP\n应用场景：消息队列（使用BRPOP可以阻塞读取，节省CPU开销）\nps：使用List作为消息队列存在缺陷，具体见Stream\nHash 底层数据结构：\n7.0之前：元素个数和元素大小小于阈值时使用压缩列表ziplist，否则使用哈希表 7.0之后：使用了 listpack 替代压缩列表 常用命令：HSET HMSET HDEL HLEN HGETALL HINCREBY HSCAN\n应用场景：缓存对象\nps1：用Hash做缓存相比于string的优势：便于维护、节省空间\nps2：如何使用ziplist存储的？将所有的field-value连续存储，列表结构类似：[field1, value1, field2, value2]\nSet 特性：无序、不可重复、支持多个集合取交集、并集、差集\n底层数据结构：集合中的元素都是整数且元素个数小于阈值时使用整数集intset，否则使用哈希表\n常用命令：SADD SREM SMEMBERS SCARD SISMEMBER SRANDMEMBER SPOP SINTER SUNION SDIFF SSCAN\n应用场景：标签（tag）、点赞\nps：为什么选用intset而不用ziplist：插入时需保证唯一性，因而需要查找是否有相同元素，intset可以二分查找而ziplist只能遍历。\nZset 特性：不可重复、可排序\n底层数据结构：\n7.0之前：元素个数和元素大小小于阈值时使用压缩列表ziplist，否则使用跳表skiplist 7.0之后：使用了 listpack 替代压缩列表 常用命令：ZADD ZREM ZSCORE ZCARD ZRANGE ZREVRANGE ZRAN ZSCAN\n应用场景：排行榜\nps：如何使用ziplist存储的？将所有的member-score连续存储，列表结构类似：{member, score}\n四种拓展类型 BitMap bitmap并不是一个真正的类型，而是基于String类型的面向位（bit-oriented）的操作。string类型可以当作一个位数组（bit vector）\n特性：适用于数据量大的二值统计场景\n底层原理：由于string是一个字节数组，一个字节8bit，利用bit串进行计数\n常用命令：SETBIT GETBIT BITCOUNT BITPOS BITOP [AND, OR, XOR and NOT]\n应用场景：签到统计、布隆过滤器\nHyperLogLog 特性：使用极小的内存对大量数据进行不精确的去重计数，存在误差\n底层原理：通过伯努利实验 + 极大似然估计反推样本总数\n常用命令：PFADD PFCOUNT PFMERGE\n应用场景：网页UV计数\nGEO 特性：用于存储地理位置信息，并对存储的信息进行操作，包括计算两点距离以及获取区域内元素。\n底层原理：直接使用zset，使用GeoHash编码方式将经纬度转换为Zset中的元素权重分数\n常用命令：GEOADD GEOPOS GEODIST GEORADIUS\n应用场景：滴滴打车\nStream 在5.0之前，消息队列是使用list实现的，但是存在以下的几点问题：\n如果需要判断重复消息，生产者需要自行为每条消息生成一个全局唯一ID。 消息可靠性无法保证，当消费者处理时出现宕机会导致当前消息丢失。可以创建一个备份list，但无法完全保证可靠。 不支持多个消费者消费同一条消息 不支持消费组的实现，消费组：组内一个成员读取了消息，同组其他成员就不能再读取同一个消息了。 stream的出现目的就是让消息队列更加稳定可靠。\n特性：自动生成ID、支持ack确认消息的模式、支持消息持久化、支持消费组\n底层数据结构：radix tree + listpack，radix树中key值为消息ID，value是listpack结构存储的具体数据。\n常用命令：XADD XLEN [XREAD STREAM] XDEL XRANGE XREADGROUP XPNDING XACK\n应用场景：消息队列、负载均衡\n功能：\n消息保序：XADD/XREAD 阻塞读取：XREAD block 重复消息处理：Stream 在使用 XADD 命令，会自动生成全局唯一 ID； 消息可靠性：内部使用 PENDING List 自动保存消息，使用 XPENDING 命令查看消费组已经读取但是未被确认的消息，消费者使用 XACK 确认消息； 支持消费组形式消费数据 缺点：\n可靠性：在以下 2 个场景下，都会导致数据丢失： AOF 持久化配置为每秒写盘，但这个写盘过程是异步的，Redis 宕机时会存在数据丢失的可能 主从复制也是异步的，主从切换时，也存在丢失数据的可能。 可堆积：队列长度超过上限后，旧消息会被删除，只保留固定长度的新消息。 ps：发布/订阅机制为什么不可以作为消息队列？1.不具备数据持久化的能力（不会记录到日志文件）2.发后即忘 3.消息积压会导致消费端断开\n","permalink":"http://localhost:1313/posts/redis/data_structure/","summary":"详细分析了redis底层数据结构，由下到上认识了基本数据类型以及优点缺点。","title":"redis 数据结构"},{"content":"redis 全景图 两维度三主线 两维度：应用维度，系统维度 三主线（三高）：高性能，高可靠，高可扩展 问题画像图 内部基本架构 CS架构 Redis采用了CS（客户端-服务器）架构。客户端发出请求，例如GET、SET、SUBSCRIBE等命令，服务器接收这些命令并执行，然后将结果返回给客户端。\n服务器：通常指的是运行Redis服务软件的计算机，这个服务会监听特定的端口，接收来自客户端的连接请求。服务器存储并管理所有Redis的数据，并处理所有与数据相关的命令请求。 客户端：向Redis服务器发送命令并读取服务器返回结果的一方。客户端可以是运行在命令行的Redis cli工具，或者是在应用程序中使用的Redis库。 举个例子，如果你在你的电脑上安装了Redis，并在命令行中输入redis-cli来启动Redis的命令行客户端，那你就是客户端，你正在操作的这个计算机就是服务器。\n这样，通过客户端和服务器的相互作用，Redis可以作为一个中间人，允许应用程序存储和检索值，或者在一组订阅者之间发布和接收消息。\nCS架构优势 分布式：在CS架构中，任务和资源分布在不同的机器上，可以利用网络将分散的资源和处理能力整合在一起，提高整体性能。 易维护：服务器集中处理数据和业务逻辑，方便进行数据备份和恢复，以及版本更新和业务变更。 可扩展性：通过增加服务器的数量，可以方便地扩展系统的处理能力，满足业务的增长需求。 安全性：服务器可以实行集中控制和安全保护，可以进行有效的安全管理，如防火墙、访问控制等。 便于管理：管理员只需要在服务器端进行维护，用户不需要对软件进行复杂的设置和管理。 数据一致性：所有的数据都存储在服务器端，客户端只是获取和操作数据，因此可以很好地保证数据的一致性。 负载均衡：服务器可以根据负载情况，动态分配任务给客户端，促进资源的合理利用。 但同时，CS架构也有其局限性，例如服务器是系统的瓶颈，服务器出问题可能影响到所有的客户端。因此，在实际应用中需要权衡各种因素，选择适合的架构。\n","permalink":"http://localhost:1313/posts/redis/overall/","summary":"从宏观俯瞰redis","title":"redis 全景图"},{"content":"select 关键字 参考 draveness select关键字\n设计思想 与操作系统中多路复用模型相似，同时监听多个 channel 状态，等待变为可读或可写。\n与 switch 关键字的控制结构相似，只不过 select 的 case 表达式必须是 channel 的收发操作。\nselect 两个特别之处：\n在 channel 上可以进行非阻塞的收发操作 在多个 case 同时满足收发状态时，随机执行其中之一 非阻塞收发 当 select 中存在 default 分支时，只需要知道其他分支中的 channel 收发状态是否就绪，如果全部未就绪就会执行 default 分支上的操作。\n因此，在其他分支上对于 channel 的读写操作都是非阻塞的，具体实现见channel这一部分。\n随机执行 在 runtime.selectgo 开头，通过 runtime.fastrandn 函数随机打乱case的轮询顺序 pollOrder。\n目的：避免饥饿问题的发生\n实现原理 每个case对应一个 runtime.scase 结构体，结构体中保存着收发操作目标 channel 和接收到的数据要放到的内存地址（等号左边变量指针）。\n编译期间 在编译过程中，会根据 case 的不同情况对控制语句进行优化：\n不存在任何case：调用 gopark 永久阻塞当前 goroutine。\n只存在一个case：改写为 if 条件语句判断是否为 nil channel，是则永久阻塞当前 goroutine，否则正常执行 channel 的收发操作。\n一个普通case，一个default：改写为 if-else条件语句，if 中调用非阻塞channel收发函数，当返回的状态为未就绪时，执行 else 中的逻辑。\n多个case：将所有case转化为 runtime.scase 结构体数组，调用 runtime.selectgo 函数确定可执行的case对应的结构体。\n运行时期间 运行时主要就是执行 runtime.selectgo 这个函数，目的是确定可执行的scase结构体。\n主要步骤：\n初始化两顺序：随机打乱轮询顺序（避免饥饿），按照channel地址排序确定加锁顺序（避免死锁）。如果 case 中 channel 为 nil，则在后续的轮询中忽略这个case（不加入顺序切片）。 给 select 中的所有 channel 加锁。 按轮询顺序遍历所有 case 查找存在的准备就绪的 channel，如果有，立刻执行，解锁全部channel后返回。 对于非阻塞操作（存在 default），直接全部解锁并返回。 按加锁顺序遍历所有 case，将当前 goroutine 包装为 sudog，加入每一个channel的等待队列中，此外还会附着在当前 goroutine 的 waiting 链表末尾。 调用 gopark 暂止当前 goroutine，解锁所有 channel，等待有就绪事件时被唤醒。 给 select 中的所有 channel 加锁。 按加锁顺序遍历所有 case，找到就绪的 case，将其他case中没有被用到的 sudog 从 channel 等待队列出队并释放掉。 解锁所有 channel，返回选中的case的索引，从而可以执行对应的逻辑。 不按顺序加锁为什么会导致死锁？ 在并发加锁的时候会出现问题。例如：两个goroutine中分别有两个select，g1按顺序对 chan1、chan2 加锁，g2按顺序对 chan2、chan1 加锁。\n由于select需要将内部所有channel都加锁才会执行任务，当按g1-chan1、g2-chan2顺序加锁时，g1-chan2、g2-chan1会阻塞，导致死锁。\nnil channel 的用处与原理 在上述步骤1中，确定 case 轮询顺序时不会包括channel为nil的case，从而在后续的遍历过程中不会涉及这种case，因此可以说 nil channel 的 case 被永久阻塞了。\n将某个case的channel置为nil可以禁用当前分支，而且其他的分支不会受影响。这样做可以在for...select...这种结构中避免一个已经完成所有任务的channel反复被检查，节约了系统资源。\n","permalink":"http://localhost:1313/posts/golang/select/","summary":"select关键字用于多路通信和同步，可以同时监听多个channel的状态，并等待其中一个channel就绪。它具有非阻塞收发和随机执行的特点，通过编译和运行时的优化与机制来实现多路通信。select是Go中处理并发通信和同步的强大工具。","title":"Go select关键字"},{"content":"channel 参考 Draveness channel\nGolang Concepts: Nil Channels\nGo 程序员面试笔试宝典 通道\n设计原理 Do not communicate by sharing memory; instead, share memory by communicating.\n不要通过共享内存来通信，而要通过通信来实现内存共享。\n数据结构 type hchan struct { qcount uint // 环形队列中的元素总数 dataqsiz uint // 环形队列大小 buf unsafe.Pointer // 环形队列底层数组，只针对有缓存channel elemsize uint16 // chan元素大小 closed uint32 // chan是否关闭 elemtype *_type // chan元素类型 sendx uint // 环形队列中的发送索引 recvx uint // 环形队列中的接收索引 recvq waitq // 接收等待队列，双向链表 sendq waitq // 发送等待队列，双向链表 lock mutex // 保护hchan中所有字段 } 主要组成 = 环形队列 + 接收阻塞队列 + 发送阻塞队列 阻塞队列 阻塞队列 runtime.waitq 是遵循先进先出原则的双向链表，链表元素是将 goroutine 和待发送或待接收数据地址等进行封装后的 runtime.sudog\nsudog的必要性：g和并发对象是多对多关系，一个g可能在很多等待队列中。 sudog is necessary because the g ↔ synchronization object relation is many-to-many. A g can be on many wait lists, so there may be many sudogs for one g; and many gs may be waiting on the same synchronization object, so there may be many sudogs for one object. 创建 通过 make 函数创建，根据声明的元素类型和缓冲区大小选择不同的初始化策略：\n无缓冲区：只分配 runtime.hchan 内存空间\n有缓冲区：\nchannel中存储的不是指针类型：为 runtime.hchan 和 hchan.buf底层数组分配一块连续的内存空间 channel中存储的是指针类型：分别为 hchan 和 底层数组 分配内存空间 channel是在堆上分配的，目的是为了goroutine之间的通信（因为每个G都有自己的栈）\n向channel发送 三种特判：\n如果 hchan 是 nil，那么会调用 gopark 挂起当前goroutine，直接返回。 非阻塞发送（select中）如果无缓冲区或者满了，那么直接返回。 如果channel已经关闭，panic。 三种情况：\n接收阻塞队列不为空 缓冲区有空闲 缓冲区无空闲 特判3和三种情况是要加锁的（hchan.lock）。 接收阻塞队列不为空 channel可能的两种状态：无缓冲区channel、缓冲区为空的channel 取队头元素，获取sudog中保存的待接收数据地址，直接将发送的数据copy到待接收地址上，并且将sudog中保存的暂止状态的 goroutine 标记为 Grunnable 状态，放到P的 runnext位置上等待执行。解锁直接返回。\n如何保证待接收数据的对象不被回收？ 答：保存到等待队列前调用 runtime.KeepAlive 禁止被收集器回收。 优点 降低了内存拷贝的性能损耗\n暂止的goroutine不需要再获取channel锁来操作缓冲区，提高了速度\n缺点 一个运行的G直接对另一个G的栈进行了修改，事实上违反了GC中关于goroutine栈是各自独有的假设，因此需要在修改过程中加写屏障。\n缓冲区有空间 获取 hchan.sendx 索引，将数据拷贝到缓冲区索引位置，更新索引。解锁直接返回。\n缓冲区无空间 两种情况：无缓冲区channel、缓冲区已满channel 组装包含当前g以及相关信息的 sudog，将其入队，并调用 gopark 暂止当前的g，等待被唤醒。\n从channel接收 三个特判\n如果 hchan 是 nil，对于非阻塞接收直接返回；而对于阻塞接收会调用 gopark 挂起当前goroutine，直接返回。 非阻塞接收（select中）如果无缓冲区或者缓冲区为空，清除待接收指针对应的数据（将等号左边等待赋值的变量置为零值），直接返回。 如果channel已经关闭，并且缓冲区中没有数据，清除待接收指针对应的数据，直接返回。 三种情况\n发送阻塞队列不为空 缓冲区有数据 缓冲区无数据 同样，特判3和三种情况是要加锁的（hchan.lock）。 发送队列不为空 如果是无缓冲区channel，那么直接将队头元素中保存的值copy给待接收变量。\n否则，说明channel缓冲区已满，需要获取缓冲区中 hchan.recvx 索引对应的值，并取出发送队列头元素，将刚刚缓冲区索引对应位置的值用元素中保存的值代替，并更新 recvx 索引到下一个位置，唤醒这个暂止的发送goroutine，解锁直接返回。\n简而言之，就从缓冲区取一个值并从发送队列递补。\n缓冲区有数据 根据 hchan.recvx 索引从缓冲区取一个值赋给等号左边待接收变量，更新索引，解锁直接返回。\n缓冲区无数据 组装包含当前g以及相关信息的 sudog，将其入队，并调用 gopark 暂止当前的g，等待被唤醒。\n关闭 当关闭一个 nil channel 或者关闭一个已经关闭的 channel 时，panic。\n将接收和发送队列中所有等待中的goroutine取出来唤醒。其中发送队列的所有goroutine都会panic。\nnil channel 有什么用？ 可以用来动态地控制哪些 case 可以在 select 中被执行。通过将 channel 设置为 nil 临时禁用通道交互。具体见select关键字\nchannel 引发的资源泄露 对于一个channel，如果没有任何的goroutine引用，那么会进行GC将其回收。\n而出现资源泄露是因为channel处于满或空的状态，一直得不到改变，发送或接收的goroutine一直在等待队列得不到释放。\n应用 与 Timer 结合可以实现超时控制和定时任务 解耦生产方消费方，异步起几个 worker 通过 channel 读取任务 通过缓冲区大小限制并发数 runtime/chan.go 出现的内存对齐操作 maxAlign = 8 hchanSize = unsafe.Sizeof(hchan{}) + uintptr(-int(unsafe.Sizeof(hchan{}))\u0026amp;(maxAlign-1)) 作用：对齐内存，低3位保证为0，即hchanSize为8的倍数\nmaxAlign-1为掩码，-int(unsafe.Sizeof(hchan{}))为补码，\u0026amp;运算只保留低3位bit，这样在hchanSize低3位上总为0\n例子\n表达式 例1 例2 unsafe.Sizeof(hchan{}) 01101 00101 -int(unsafe.Sizeof(hchan{})) 10011 11011 maxAlign-1 00111 00111 uintptr(-int(unsafe.Sizeof(hchan{}))\u0026amp;(maxAlign-1)) 00011 00011 hchanSize 10000 01000 ","permalink":"http://localhost:1313/posts/golang/channel/","summary":"文章深入讲解Go语言的channel原理，包括其创建、发送、接收和关闭四个操作，并解答了nil channel的应用场景和channel可能导致的资源泄露问题，对内存对齐操作进行了说明。","title":"Go channel原理"},{"content":"I/O模型和网络轮询器netpoll 参考 Java并发之BIO NIO AIO IO多路复用的区别\n网络轮询器\n小林coding\n面试官：请你谈谈关于IO同步、异步、阻塞、非阻塞的区别？\n潘少：Go netpoller 原生网络模型之源码全面揭秘\n五种I/O模型 同步阻塞 I/O（BIO）、同步非阻塞 I/O（NIO）、信号驱动 I/O（SIGIO）、异步非阻塞 I/O（AIO） 、I/O 多路复用（I/O multiplexing）\nI/O两阶段：等待数据准备、将数据从内核（用户程序）拷贝到用户程序（内核），见下图；\n阻塞/非阻塞：主要看第一步，线程在资源就绪之前是否需要等待\n同步/异步：主要看第二步，请求线程在数据用户态内核态之间的拷贝期间是否被阻塞\nlinux中文件描述符是什么：文件描述符（file descriptor）就是内核为了高效管理这些已经被打开的文件所创建的索引，其是一个非负整数（通常是小整数），用于指代被打开的文件，所有执行I/O操作的系统调用都通过文件描述符来实现。 同步阻塞I/O（BIO） 最常见的IO模式，例如读写文件时执行read write系统调用。\n从开始执行系统调用开始，用户线程就会进入阻塞状态直到I/O操作结束。\n优点：简单，线程等待期间基本不会占用CPU资源 缺点：高并发下大量线程被阻塞，导致内存和线程切换开销巨大 同步非阻塞I/O（NIO） 用户程序会不断轮询发起系统调用，直到内核中资源准备就绪，再完成I/O操作。\n优点：可以在等待过程中执行其他任务，提高 CPU 利用率 缺点：需要不断轮询占用大量 CPU 时间，且轮询间隔可能会放大响应延迟，降低整体数据吞吐量 异步非阻塞I/O（AIO） 用户线程只需要负责向内核发送一个系统调用请求，等待数据就绪和数据拷贝的过程都由内核来完成，完成后内核通过信号或回调函数通知用户线程I/O操作完成。\n优点：减少了 NIO 中轮询占用的 CPU 资源，且减少了响应延迟 缺点：复杂，回调函数占用系统资源，使用信号通信多平台可移植性差 多路复用 事件驱动机制，使用特定的系统调用（select、poll、epoll等）阻塞地监听一组文件描述符，当文件描述符就绪（状态转变为可读或者可写）时，通知程序继续进行相应的I/O操作。\n多路复用也属于同步策略，与NIO比较类似，只不过是由特定的系统调用负责去轮询资源是否就绪，而不是用户线程亲力亲为\n为什么属于同步策略：特定系统调用只负责告诉用户程序资源是否就绪，而（在read调用中）数据从内核复制到用户内存的操作还是由用户程序发送系统调用触发\n为什么可以处理大量连接：NIO一个请求需要一个线程去处理，而多路复用则是做到了多路请求复用一个线程，大大减少了资源占用\n优点：可以同时处理大量连接，减小系统开销，响应性能好 缺点：select和poll都使用线性结构（数组和链表）存储文件描述符，需要拷贝到内核内存开销大，返回就绪事件需要遍历时间复杂度高，select还有数组大小限制 此外，由于特定系统调用是轮询的并且拷贝数据的时候用户线程也是阻塞的，所以多路复用只有在处理高并发请求的时候才能体现性能优势。\nepoll epoll解决了select、poll存在的内存开销和时间复杂度问题\nepoll使用红黑树的数据结构追踪所有待检测的文件描述符，每次只需要加入一个待检测的文件描述符，而不像select、poll一样每次操作都穿入整个集合，加入时间复杂度O(logn)\n维护wait链表来记录就绪事件，而不是遍历整个集合\n事件驱动 边缘触发：当被监控的文件描述符上第一次出现事件就绪时，通知用户程序 水平触发：当被监控的文件描述符上存在就绪事件，通知用户程序 go中的网络轮询器netpoll netpoll的实现原理是多路复用+NIO（见最后总结部分），go中使用原生网络模型构建tcp server时，网络模型底层就是基于netpoll实现的\ngo中使用 runtime.pollDesc 封装操作系统的文件描述符，空闲描述符以链表形式存储。\n五个函数\nfunc netpollinit() // 创建实例 func netpollopen(fd uintptr, pd *pollDesc) int32 // 注册fd func netpoll(delta int64) gList // 等待事件操作 func netpollBreak() // 停止epoll func netpollIsPollDescriptor(fd uintptr) bool // 判断fd是否被netpoll使用 go中针对不同平台采用了不同版本的网络轮询模块，linux-epoll，darwin-kqueue\u0026hellip;下面以epoll为例。\n初始化 netpollinit 函数只会在初始化网络IO或文件IO或计时器时调用一次（sync.Once来实现），其作用是：\n创建一个新的epoll文件描述符 创建一个用于通信的管道 将 netpollBreakRd 通知信号量封装成 epollevent 事件结构体注册进 epoll 实例。 netpollBreakRd 是一个用于传递控制信号的管道，监听这个事件的目的是为了实现网络轮询的终止。 然后，会批量初始化pollDesc并调用 netpollopen 函数注册轮询事件监听文件描述符（如net包中的listener fd）到epoll实例\n等待事件 当在文件描述符上执行读写操作时（如net包中对请求Read或Write），会调用runtime.poll_runtime_pollWait：\n检查描述符状态，如果fd不可读/写，则会执行gopark让出当前线程，将Goroutine转换到休眠状态。 将goroutine保存在pollDesc中（pollDesc是go封装的文件描述符），将pollDesc加入epoll底层基于红黑树的阻塞队列eventpoll.rbr中。 等待描述符变为可读或可写之后通知运行时，唤醒休眠的Goroutine。 轮询等待 调用netpoll函数获取需要唤醒的goroutines：\n根据传入的参数确定epoll系统调用需要等待的时间（永久阻塞、非阻塞、有限期阻塞）。 通过epollwait 系统调用获取待处理事件的数量。 根据返回的待处理事件数量将eveentpoll.rdllist就绪链表中的额所有pollDesc中存储的Goroutine加入运行队列（本地或全局）等待调度器的调度。 原生网络库 调度过程 netpoll的价值 前置知识：\n当一个运行的 G 发生系统调用时，不只是 G 会被阻塞，执行该 G 的 M 也会被阻塞， go并发的优势就在于将任务的执行载体细化到了用户级的 G 不使用netpoll会存在的问题：\n对于I/O密集型任务，大量系统调用会导致 M 频繁的挂起、新建、唤醒，背离了golang运行时调度的设计思想，即程序的控制权掌握在用户进程而不是内核，导致无法使用强大的调度器进行并发调度了 netpoll的价值：\n发生系统调用时，运行时将 G park住并将包装后的描述符加入epoll等待就绪，避免了因为陷入内核态导致 M 被阻塞，因此这个 M 还可以继续配合调度器运行新的 G。 总结 为什么说 netpoll 是 NIO + 多路复用？ NIO指的是对于线程 M，发生I/O操作时不会因为系统调用而被阻塞\n多路复用指的是操作系统层面，使用提供的 epoll、kqueue等多路复用技术\nnetpoll优势 保证了在I/O密集型服务中，go依旧可以实现在运行中进行用户级调度，提高了并发能力，减少了系统资源占用\nnetpoll存在的问题 服务器在短时间内建立了海量连接，会导致创建大量的goroutine，过多的消耗系统资源。\n解决办法：Reactor模型\n","permalink":"http://localhost:1313/posts/golang/netpoll/","summary":"Netpoll是Go语言中的网络轮询器，采用NIO和多路复用的机制，保证了在I/O密集型服务中，能够进行用户级调度，提高并发能力，减少系统资源占用。然而，如果服务器在短时间内建立了大量连接，可能导致创建过多的goroutine，消耗系统资源。","title":"I/O 与 Go 网络轮询器"},{"content":"init 参考 一文读懂 Golang init 函数执行顺序\n特点 可选的，无入参和返回值，自动执行不能被调用，没有数量限制\n调用顺序 同一包 同一源文件 同一文件中有多个init函数，从上到下执行\n不同源文件 根据文件名的字典序确定执行顺序\n不同包 无依赖关系 按照main.go中import顺序分别调用包的init，最后再调用main包的init。如果没有被import，则包中的init不会执行\n无依赖关系指包之间没有依赖关系，在main文件中被import，并使用占位符\u0026rsquo;_\u0026lsquo;接收：import _ \u0026quot;proj/a\u0026quot; 有依赖关系 从依赖最底层的包一层层向上调用，例如：main \u0026gt; a \u0026gt; b \u0026gt; c，init顺序应为 c \u0026gt; b \u0026gt; a \u0026gt; main\n项目初始化流程 包内：常量 \u0026gt; 变量 \u0026gt; init\n整体：从依赖最底层的包逐层向上初始化\n写过的bug 同一个包，主函数读取命令行参数确定全局变量 isProd 的值，而redis和pg客户端的初始化放在了各自的init函数中，导致全局变量的改变没有被init函数使用。（即使当读取参数的代码放在main.go的init中，字典序小于main的所有go文件中的init函数都不能感知到main.go中读取到的参数）\n","permalink":"http://localhost:1313/posts/golang/init/","summary":"介绍了init函数的特点、调用顺序和项目初始化流程。","title":"Go init函数"},{"content":"Collector 参考 draveness垃圾收集器\ngo语言原本-垃圾回收\ngo程序员面试笔试宝典-垃圾回收器20问\n渐进式垃圾回收-CSDN\n奇伢云存储-golang混合写屏障\n设计原则 垃圾收集算法 追踪式：从根对象出发，一步步扫描回收对象\n引用计数式：对象自身包含一个被引用的计数器\n根对象是什么：全局变量、执行栈、寄存器 分代式 根据对象的存活时间进行分类。（JAVA：年轻代、老年代、永久代）\n优点：相比传统标记清除 STW 更小，分代选取不同垃圾收集策略，效率高。 缺点：额外开销，内存占用，且golang中编译器会通过逃逸分析将大部分新生对象存储在栈上（栈直接被回收），因此没必要。 标记清除 从根对象出发，将确定存活的对象进行标记，并清扫可以回收的对象。\n优点：简单 缺点：需要长时间 STW 三色标记清除 优化的标记清除算法，将对象分为三类：黑、白、灰，保证与用户程序并发执行时程序的正确性（对象不会被错误回收）\n优点：利用并发的优势，大幅减小 STW 时间 缺点：复杂，需要配合屏障技术，否则会出现悬挂指针问题（一个还在生命周期的对象被错误回收了） 收集策略 并发和增量策略都需要配合屏障技术保证回收的正确性\n增量 增量式收集是将一个完整的 GC 过程切成多个更小的 GC 时间片，穿插在用户程序之间执行，在执行时仍需要 STW。 目的：减少了每次 STW 用户程序需要等待的时间，将总等待时间分散到多个时间片中。 并发 利用多核优势在部分阶段与用户程序并行执行 目的：减少了总的 STW 时间 三色抽象 概念 三类对象：\n白色对象：未被回收器扫描到的对象。回收开始阶段全部对象都为白色，扫描结束后存在的白色对象为不可达对象。 灰色对象：已经被回收器扫描但存在指针指向一些还没被扫描白色对象。（波面） 黑色对象：对象本身以及其所有指针指向的对象都被扫描过，确定存活的对象。 三色不变性 想要在并发或者增量的标记算法中保证正确性，我们需要达成以下两种三色不变性（Tri-color invariant）中的一种：\n强三色不变性 — 黑色对象不会指向白色对象，只会指向灰色对象或者黑色对象；（黑色不能指向白色） 弱三色不变性 — 黑色对象指向的白色对象必须包含一条从灰色对象经由多个白色对象的可达路径；（黑色可以指向白色，前提是必须要有另外的灰色对象直接或间接指向这个白色） 为什么要三个色？ 为了配合屏障技术，保证在并发或者增量执行时不会出现悬挂指针（本来不应该被回收的对象却被回收了）。\n具体来说，就是要保证达成至少一种三色不变性。\n屏障技术 是一种同步机制，使用户程序在进行指针写操作时，能够“通知”回收器，进而不会破坏三色不变性，保证在增量或并发回收时程序的正确性\nDijkstra 插入写屏障 在修改引用时，将新指向的对象置灰。换句话说，目的是将有存活可能的对象都标记成灰色\nwritePointer(slot, ptr): shade(ptr) *slot = ptr slot是代码中的接收位置（destination），ptr是被指向的对象，ptr需要进入slot中（goes into the slot）\n如图所示，用户程序修改A的指针，将其指向C，这时写屏障触发将C标记为灰色，在收集器启动时，将从B C 两个对象出发进行扫描。\n保证了强三色不变性。 是一种保守的屏障技术。 可能会遗留不再存活的对象，如图中的B，或者当图中第二三步间重新改变A指向B后的C。 栈上对象也需要保证回收正确性，因此需要为栈对象添加写屏障或STW并重新扫描栈上对象，GO1.7版本以前采用了后者。 Yuasa 删除写屏障 在删除引用时，如果原来指向的对象为白色，则将其置灰。目的是间接保留原有的引用关系使得扫描可以进行下去。\nwritePointer(slot, ptr) shade(*slot) *slot = ptr 第二步由于B本来就是灰色的，所以不用改；第三步将B指向C的引用删除，这时如果不将C染成灰色，后续收集器就不会扫描白色的C和D，于是CD被错误的回收了。\n如果第二步 A 指向了另外一个独立的白色对象 E 岂不是出错了？ 这种情况不可能出现，原因见下边第三点。 保证了弱三色不变性。 同样会遗留不再存活的对象。 【very important！】指针更新一定是从一个活动对象指向另一个活动对象，因为非活动对象是没有任何指针引用的，用户程序不可能再引用到它。换句话说，不可能新增一个指向孤立的白色对象的引用 混合写屏障 插入和删除写屏障都存在的问题：对栈对象加屏障严重拖累性能，尤其对于go来说（goroutine可以很多）\n如果不对栈对象加屏障，那么会导致的问题：\n只用插入屏障：一个黑色栈对象指向一个白色堆对象，并且原来灰色对象指向该白对象的指针被删除了，的会导致悬挂指针问题，需要标记后 STW 重新扫描所有栈对象。 只用删除屏障：标记结束后不需要重扫栈，但是图1-\u0026gt;图2过程会导致悬挂指针。 因此，选择混合写屏障（插入+删除）可以避免指针悬挂问题，且不用标记结束后重新扫栈，提高了效率。\nwritePointer(slot, ptr): shade(*slot) if current stack is grey: shade(ptr) *slot = ptr 此外，要将创建的所有新对象都标记成黑色，防止新分配的对象被错误地回收。\n垃圾回收的触发 触发阈值 通过环境变量 GOGC 设置，默认为100，即增长 100% 的堆内存才会触发 GC。\n不同于进入 STW 以后进行垃圾收集的方法，并发收集器并不能等到堆内存达到触发阈值时才开始运行，因为在 GC 期间还会有用户程序在分配新的堆内存。\n所以使用 Pacing 算法计算触发 GC 的最佳时间，使得收集（标记扫描）结束时堆内存近似达到阈值\n触发时机 可以分为三种情况：\n后台定时检查，当一定时间内没有触发（默认2min），就会触发新的 GC 循环。 用户程序手动触发，如果当前没有开启 GC ，则触发新的循环。 申请内存时根据堆大小触发，当堆内存达到阈值时，则触发新的循环。 后台触发 运行时在应用程序启动时创建的一个专门用于触发垃圾收集的goroutine，大多数时间是在休眠的，由 sysmon 在满足触发条件的时候唤醒。\n手动触发 用户程序通过 runtime.GC 函数主动通知运行时执行\n分配内存 在分配微对象和小对象的内存时，如果mcache中找不到空闲内存单元，则会向下层申请mspan并触发 GC。\n在分配大对象之前，一定会先触发 GC 回收内存。\nGC 实现 核心函数 runtime.gcStart，开启一个 GC 循环。\n垃圾回收三状态 垃圾回收器通过 _GCoff、_GCMark 和 _GCMarktermination 三个标记来确定写屏障状态\n_GCoff ----\u0026gt; _GCmark ----\u0026gt; _GCmarktermination /\\ | |___________________________________| _GCoff：清理状态; 写屏障关闭\n_GCmark：标记状态；写屏障开启\n_GCmarktermination：标记终止状态；写屏障开启\n垃圾回收四阶段 清理终止阶段（STW）\u0026ndash;\u0026gt; 标记阶段 \u0026ndash;\u0026gt; 标记终止阶段（STW）\u0026ndash;\u0026gt; 清理阶段\n清理终止阶段 状态 _GCoff；所有处理器在这时进入安全点；对上个垃圾回收阶段进行一些收尾工作（例如清理缓存池、清理已经被标记的内存单元、清理标记等等）， 以及完成进入标记阶段前的一些准备工作（启动后台标记任务、更新 GC 组件中相关变量），进入 STW 状态。\n标记阶段 状态切换至 _GCmark；扫描栈上、全局变量等根对象并将它们加入队列，将所有P的微分配器中的对象置灰，以及开启写屏障，退出 STW 状态，程序恢复执行，后台标记任务开始恢复进行。\n标记终止阶段 当所有的标记任务都完成后，进入 STW 状态，状态切换至 _GCmarktermination；以及完成标记的收尾工作（统计数据、重制状态等），关闭写屏障。\n清理阶段 状态切换至 _GCoff；退出 STW 状态，后台并发清理所有的内存管理单元\n总之，在访问公共资源时需要进入 STW 状态，进行扫描标记时要开启写屏障 如何暂停和恢复程序 暂停：抢占所有的处理器P，将他们更新至 _Pgcstop 状态\n恢复：依次唤醒所有的处理器\n后台标记模式 在步骤1（标记阶段开始前），运行时会为全局每个处理器创建一个用于后台标记任务的 Goroutine（遍历 runtime.allp），这些 Goroutine 都会与P绑定成为 node结构体 被加入 runtime.gcBgMarkWorkerPool 线程池并陷入休眠等待调度器的唤醒；程序恢复后在调度循环中寻找可运行的G runtime.findrunnable 的时候被从池中取出调度到P上运行。\n这些后台标记任务被称为 gcMarkWorker\nworker有三种模式：\ngcMarkWorkerDedicatedMode：处理器专门负责标记对象，不会被调度器抢占 gcMarkWorkerFractionalMode：当垃圾收集的后台CPU 使用率达不到预期时（默认为 25%），启动该类型的工作协程帮助垃圾收集达到利用率的目标 gcMarkWorkerIdleMode：当处理器没有可以执行的 Goroutine 时，它会运行垃圾收集的标记任务直到被抢占 在调度循环中会根据全局处理器的个数以及垃圾收集的 CPU 利用率确定worker的工作模式\n三种模式相互协同保证了标记的速率，在到达堆内存触发阈值前完成标记任务\n辅助标记 在并发标记阶段期间，当 Goroutine 调用 runtime.mallocgc 分配新对象时，该函数会检查申请内存的 Goroutine 是否处于入不敷出的状态：它遵循一条非常简单并且朴实的原则，分配多少内存就需要完成多少标记任务。\n假设如果有个Goroutine一直大量的分配内存，这样有可能永远都扫描不完所有对象，或者当扫描结束时堆内存已经到了一个很夸张的大小。所以辅助标记就是为了避免这种情况。\n后台标记模式和辅助标记区别：\n后台标记是通过并发加快标记的速度，以保证到达触发阈值时完成标记任务；\n辅助标记是在后台并发标记期间，当前 goroutine 分配内存时，通过控制分配与标记的“动态平衡”，保证到达触发阈值时完成标记任务；\n观察程序GC的四种方法 GODEBUG GODEBUG=gctrace=1 ./main 输出\ngc 1 @0.000s 2%: 0.009+0.23+0.004 ms clock, 0.11+0.083/0.019/0.14+0.049 ms cpu, 4-\u0026gt;6-\u0026gt;2 MB, 5 MB goal, 12 P scvg: 8 KB released scvg: inuse: 3, idle: 60, sys: 63, released: 57, consumed: 6 (MB) 其中，gc开头的是用户代码向运行时申请内存产生的垃圾回收，scvg开头的是运行时向操作系统申请内存（归还内存）产生的垃圾回收\ntrace func main() { f, _ := os.Create(\u0026#34;trace.out\u0026#34;) defer f.Close() trace.Start(f) defer trace.Stop() (...) } 通过在代码内调用trace API 来在网页内进行可视化观察\ndebug.ReadGCStats func printGCStats() { s := debug.GCStats{} debug.ReadGCStats(\u0026amp;s) fmt.Printf(\u0026#34;gc %d last@%v, PauseTotal %v\\n\u0026#34;, s.NumGC, s.LastGC, s.PauseTotal) } 可以搭配计时器实现对指定指标的定时监控\nruntime.ReadMemStats func printMemStats() { s := runtime.MemStats{} runtime.ReadMemStats(\u0026amp;s) fmt.Printf(\u0026#34;gc %d last@%v, next_heap_size@%vMB\\n\u0026#34;, s.NumGC, time.Unix(int64(time.Duration(s.LastGC).Seconds()), 0), s.NextGC/(1\u0026lt;\u0026lt;20)) } 和上边方法一样，只不过这里直接调用了运行时的结构体\nGC调优 调优思想：优化内存的申请速度（控制），尽可能的少申请内存（减少），复用已申请的内存（复用），减少GC触发频率（频率）\n控制：提高赋值器CPU使用率，减少花费在调度器的等待上的时间（使用 sync.WaitGroup 等并发策略避免系统中存在过多的goroutine，因为每个goroutine都会占用内存）。\n减少：合理使用数据结构\n复用：sync.Pool 等池化技术\n频率：调整GOGC上限（GOGC=1000 ./main）\n","permalink":"http://localhost:1313/posts/golang/collector/","summary":"该文章介绍了Go语言中的垃圾回收机制，包括算法、策略和优化技术。","title":"Go 垃圾收集器"},{"content":"git 结构 工作区：实际代码目录\n版本库：.git目录，主要分为两部分：\n暂存区（stage/index）：保存 git add \u0026lt;file\u0026gt; 添加的更改 本地仓库（respository）：保存了项目的所有历史记录和元数据（上图中的objects） HEAD HEAD是一个指针，指向某个提交，用来确定当前工作目录的状态和所在位置\ngit rev-parse HEAD 查看HEAD指向的提交Hash\nbranch 使用branch进行“版本控制”而不是进行“开发”，branch只是对仓库中所有commits进行管理排序，而不是将代码“提交”到分支。\n0 \u0026lt;- 1 \u0026lt;- 2 \u0026lt;-------------------- master \\ 3 \u0026lt;- 4 \u0026lt;- 5 \u0026lt;- 6 \u0026lt;------ A \\ 7 \u0026lt;- 8 \u0026lt;- 9 \u0026lt;-- B 在如上所示的仓库中，master分支包括0～2提交，A分支包括0～6，B分支包括0～9\nmerge 将多个开发历史合并（Join two or more development histories together）\n主要有两种merge方法：\n3-way merge：创建一个“合并提交”作为两个分支的交点 Fast forward merge：只改变分支指针（适用于当前分支和目标分支位于一条线性历史上，etc. rebase过后） 优势：非破坏性操作，方便协作；可以查看上游更改。\n缺点：大量合并提交污染分支历史。\n命令 git merge [ -ff | --no-ff | --ff-only ] \u0026lt;branch\u0026gt; [\u0026lt;target\u0026gt;] 等价\ngit checkout \u0026lt;target\u0026gt; git merge [ -ff | --no-ff | --ff-only ] \u0026lt;branch\u0026gt; \u0026lt;branch\u0026gt; 要合并的分支\n\u0026lt;target\u0026gt; branch被合并到的分支\n合并方法参数\n-ff：优先快速合并，如果两分支不是线性历史则使用 3-way merge --no-ff：使用 3-way merge --ff-only：只使用快速合并，否则直接失败 rebase 将提交重新应用到另一个基准点之上（Reapply commits on top of another base tip），称为变基（改变基准点）\n比如，当前分支是从主分支commit1节点fork出来的，此后主分支又新增了2个提交commit2和commit3，这时候 git rebase master可以将工作分支相对于主分支的“分叉点”改到commit3上。\n4\u0026#39; \u0026lt;- 5\u0026#39; \u0026lt;- 6\u0026#39; \u0026lt;---- A after rebase / 0 \u0026lt;- 1 \u0026lt;- 2 \u0026lt;- 3 \u0026lt;-------------------- master \\ 4 \u0026lt;- 5 \u0026lt;- 6 \u0026lt;------ A [abandon] 优势：避免了“合并提交”的污染；线性历史方便追根溯源\n缺点：将一个公共的分支追加到到自己的分支会导致与其他协作者的公共分支相异；看不到当前分支中并入了上游的哪些更改。\n命令 git rebase [-i] [--onto \u0026lt;newbase\u0026gt; | --keep-base] [\u0026lt;upstream\u0026gt; [\u0026lt;branch\u0026gt;]] -i 可以进行交互式操作，在文本编辑器内自定义提交.\n\u0026lt;upstream\u0026gt; 要比较的上游分支。 可以是任何有效的提交，而不仅仅是现有的分支名称。 默认为当前分支配置的上游。\n\u0026lt;branch\u0026gt; 工作分支；默认是HEAD。\n--onto \u0026lt;newbase\u0026gt; 目标分支，不指定默认为upstream\n\u0026ndash;onto 假设想把commits 7～9 rebase到master上，需要使用onto\n0 \u0026lt;- 1 \u0026lt;- 2 \u0026lt;-------------------- master \\ 3 \u0026lt;- 4 \u0026lt;- 5 \u0026lt;- 6 \u0026lt;------ A \\ 7 \u0026lt;- 8 \u0026lt;- 9 \u0026lt;-- B 错误命令：git rebase master B\noutput：Current branch xx is up to date.\n原因：此命令为“将3～9提交rebase到2后边”，当然不用变了。错误在于对branch含义的理解。\n正确命令：git rebase --onto master A B\n解释：newbase为master（commit 2），upstream 为A（确定了要copy的范围 7～9），branch为B（当前工作分支）\ncherry-pick 应用某些已有提交所引入的更改。简单来说，将某些提交应用于当前分支。\n命令 git cherry-pick A..B\nA，B是提交的Hash值，范围是 (A, B]，如果要包括A，则取A的父节点 A^\n提交 A 必须早于提交 B\n参考 atlassian的教程\ngit doc\nstackoverflow rebase onto vs rebase\n","permalink":"http://localhost:1313/posts/git/git/","summary":"文章详述了Git的基础概念如工作区、版本库、HEAD与分支，详解了合并操作包括merge和rebase方法及优劣，还讲到了rebase的\u0026ndash;onto操作和cherry-pick方法。详尽介绍了如何操作Git进行版本控制，使工作更高效.","title":"git 基础概念以及合并操作"},{"content":"内存分配 参考 draveness 内存分配器\nGo语言原本 内存分配\n例子 golang的内存分配模型可以与现实中的 零售店-经销商-工厂 模式进行类比，目的都在于可以合理的分配资源以及保证高效率\n具体而言，零售店（线程缓存mcache）拥有很多种类的商品（可以看做是空闲内存空间或者内存页），每种商品由一个售货员（mspan）进行管理（记录售出状态）和负责销售，零售店的优势就在于\n离顾客（goroutine）家很近，买东西比较便捷。当顾客需要买数量较少的商品时（微对象小对象）直接去零售店就好，如果要大批量订货（大对象）则需要直接去工厂，零售店的库存肯定不够。\n当零售店库存不足时，会向经销商（mcentral）进货。经销商拥有所有种类的商品。\n当经销商也没有足够的库存时，就要向工厂（mheap）进货。工厂仓库（arena）中存着所有已经生产完的商品，当仓库中所有的货都被预定了没有可售商品时，就会找原料供应商（操作系统）进货。\n获得原料（page）后，就会进行加工并且规划一些新仓库存放（堆扩容）。\n供货链采用三个层级的策略，减少了顾客需要亲自去工厂交易的繁琐，也保证了工厂不会将大量时间花在处理小订单上边，提高了商品流通的效率。\n设计原则 分配器 线性分配器 依靠指针的增量来确定下一个可用的内存位置，需要搭配具有拷贝特性的垃圾回收算法（标记压缩、复制回收、分代回收等）整理内存碎片，JAVA采用的分配方法，对于C/C++等直接对外暴露指针的语言不适用。\n优点：复杂度较低\n缺点：无法重用回收的内存（整理碎片之前）\n空闲链表分配器 将内存块通过指针连成链表，空闲链表分配器可以选择不同的策略在链表中的内存块中进行选择。\n优点：可重用回收的内存\n缺点：分配时需要遍历链表\n四种内存分配策略 首次适应（First-Fit）— 从链表头开始遍历，选择第一个大小大于申请内存的内存块； 循环首次适应（Next-Fit）— 从上次遍历的结束位置开始遍历，选择第一个大小大于申请内存的内存块； 最优适应（Best-Fit）— 从链表头遍历整个链表，选择最合适的内存块； 隔离适应（Segregated-Fit）— 将内存分割成多个链表，每个链表中的内存块大小相同，申请内存时先找到满足条件的链表，再从链表中选择合适的内存块； 其中，隔离适应与GO使用的策略相似： 分配思想 分级分配 它将对象划分为多个不同的大小级别（go中为微对象、小对象、大对象），并使用不同的分配器来管理每个级别的内存分配。通过分级分配，可以更好地适应不同大小对象的内存需求，提高整体的内存管理效率和空间利用率。\n注：分配器和分配思想是两个不同层面的概念，分级分配是在宏观层面为新对象选择合适的缓存存储级别，而分配器则面向底层内存分配的具体实现。 内存布局 mspan是内存管理单元，负责管理内存页和分配的对象\nmcache作用相当于缓存，它缓存了一些可以直接拿来用的内存管理单元，直接满足当前线程的内存需求，不用加锁步骤简单速度很快。\nmcentral像是一个二级缓存，按种类不同每类保存一些可以拿来用的内存管理单元，方便mcache层快速获取想要的种类。\nmheap由一个个的arena构成，arena作为虚拟内存的索引可以快速定位内存中的几页。\n注：不是一个mcache对应一个mcentral哦 go1.11以前堆内存空间是连续的，1.11中改为使用稀疏内存，runtime.heapArena 管理 pagesPerArena 个页（不同平台可能不同，但页都是8KB）\n注：go语言的页（8KB）由运行时自己定义的，与操作系统的页（大部分4KB）不同。更大的页可以减少内存管理开销（小对象可以挤一挤）、提高缓存命中率（大对象不用分太碎）以及满足对齐需求。 核心组件 mspan runtime.mspan 是GO语言内存管理的基本单元，每个mspan管理npages个GO页并持有管理的页的占用情况。每个mspan作为相同spanClass大小等级的双向链表中的一个节点相互引用。\n最小分配单位是对象（object）而不是页，object的大小是由mspan的跨度类（spanClass）决定的 对象间通过 FreeList 链表形式连接。\nFreelist是一种特殊的链表形式，通过节点值的高8字节确定下个节点的内存地址，而不是传统的保存一个Next指针字段。可以节省内存。 spanClass runtime.spanClass 是mspan的跨度类，它决定了内存管理单元中存储的对象大小和个数。\n源码中预存储了67个跨度类尺寸 (size class)，运行时中还包含1个id为0的特殊跨度类，它能够管理大于 32KB 的特殊对象。\n跨度类一共有 68 * 2 = 136 个：68个尺寸（size class），每种尺寸有两个类型（ scan 和 noscan ），noscan表示对象不包含指针所以不会被GC扫描，可以提高GC效率。\n一个spanClass由一个 uint8 表示，其中高7位是id，最后一位表示 scan 或 noscan。\n在运行时分配对象时会根据对象大小选择适合的跨度类。也正是与上述隔离适应策略的相似之处。\nmcache 线程缓存，会与处理器 P 一一绑定，主要用来缓存微小对象。每一个线程缓存都在 mcache.alloc 数组中持有 68 * 2 个 runtime.mspan，每一个数组元素都是某一个特定大小的 mspan 的链表头指针。\n拥有微对象分配器，专门管理16B以下的非指针类型的对象，\n优势：由于与P绑定，所以不会有并发问题，访问时不用加锁，效率更高。（为什么不与M绑定：M可能陷入休眠状态，期间无法复用缓存）\n初始化 在初始化处理器P时会调用 runtime.allomcache 在系统栈中使用 mheap 中的线程缓存分配器进行初始化。\nmcentral 中心缓存，每个中心缓存管理一种跨度类对应的内存管理单元（mspan），总共136个由mheap保存。这些mspan由四个 spanSet 存储，分别是：\n包含空闲对象的 partial set: [swept,unswept]\n不包含空闲对象 full set: [swept,unswept]\n垃圾回收器完成对某个内存跨度的扫描和标记操作后，该内存跨度就被认为是 \u0026ldquo;swept\u0026rdquo; 访问中心缓存中的内存管理单元需要使用互斥锁：\nmheap 页堆，以全局变量形式 runtime.mheap_ 存储，包含了长度为136（scan \u0026amp; noscan）的mcentral数组，持有的mspan列表以及所有的 heapArena（由二维数组存储）。\nheapArena GO堆在逻辑上划分为多个 heapArena ，可以把 heapArena 看作“堆块”， 每“块”里有 heapArenaBytes / pageSize 个页， 分别由 mspan 进行管理。\n对象分配 入口: runtime.newobject，new关键字的实现\n在分配时，会占有m线程，并根据对象大小和是否为指针类型（scan）确定分配逻辑：\nif size \u0026lt;= 32KB { if noscan \u0026amp;\u0026amp; size \u0026lt; 16B { 分配微对象 } else { 分配小对象 } } else { 分配大对象 } 大对象 对于size\u0026gt;32KB的对象，直接在堆上进行分配。\n流程 计算需要的pages，然后进入系统栈尝试获取新的mspan\n在分配之前先回收一部分内存以避免内存大量占用\n【mheap加锁】内存分配、mspan初始化 【mheap解锁】\n从堆上分配新的内存页和内存管理单元mspan（id为0的特殊mspan），如果堆空间不足还要考虑扩容（向操作系统申请空间），如果p0上的mspan缓存 runtime.p.mspancache 不足还要用 runtime.fixalloc 分配器分配 页内存分配使用的是全局的页分配器 runtime.pageAlloc（Radix tree） 初始化刚刚获得的mspan，并且建立mheap和mspan的关系（加入heapArena.spans）\n清理新分配的内存，进行归零操作\n返回mspan中的base地址 mspan.startAddr\n小对象 对于16B\u0026lt;=size\u0026lt;=32KB的对象和size\u0026lt;16但包含指针B的对象，，先后尝试从线程缓存、中心缓存、堆中获取mspan，如果没找到可用的则从操作系统分配新页\n流程 根据对象大小找到对应的跨度类，尝试从mcache的mspan数组缓存（mcache.alloc）中的获取对应的mspan，通过 mspan.allocCache（保存对象空闲状态的位图，原理：德布鲁因序列）寻找能够容纳当前对象的空闲对象内存空间，如果有则完成分配\n如果上一步没有找到分配空间，则需要调用 runtime.refill 向上级组件获取mspan替换已经不存在可用对象的当前mspan\n将当前的mspan（已满）放入 mcentral 的 full set\n尝试从 mcentral的 partial swept set获取一个mspan\n尝试从 mcentral的 partial unswept set获取一个mspan，并且sweep它\n尝试从 mcentral的 full unswept set获取一个mspan，并且sweep它， 如果sweep后发现没有空闲空间，则放到 full swept set中\n如果尝试100次获取后（go1.21）依旧没有找到可用的mspan， 则继续向堆申请\n向堆申请流程与大对象分配流程一致（只有span size不同） 初始化mspan字段以及初始化heapArena中该mspan的位图信息等字段 至此，寻找能用的mspan的流程终于结束了\n将新获取到的mspan保存在（mcache.alloc）中，并且返回新mspan中的空闲内存地址（freeIndex）\n微对象 对于size\u0026lt;16且不包含指针（noscan）B的对象，使用mcache中的微分配器（mcache.tiny等字段）分配。主要目标是短字符串和逃逸的临时变量\n一个tiny内存块（由maxTinySize决定，16B）中可以存多个微对象，大家挤一挤从而节省了空间。而释放内存时，要等块中所有的对象都被标记为垃圾时才会进行。\n逃逸变量：函数内部定义的局部变量，但在函数结束后仍然被其他对象或函数引用的变量（指针作为返回值等）\n为什么必须要是noscan：为了保证潜在浪费的内存量是有限的（the amount of potentially wasted memory is bounded.）\n流程 先根据对象的size进行一些内存对齐的操作，调整 mcache.tinyoffset 的内存地址 为什么要内存对齐：减少 CPU 访问内存的次数。因为 CPU 是以字长为单位访问内存的而不是字节。 看看当前tiny内存块中是否装得下（tinyoffset+size \u0026lt;= maxTinySize），装得下则直接分配，返回对象内存地址\n否则，同小对象分配流程一样获取一个mspan替换旧的tiny块，唯二不同是span size和不用对新分配的内存进行清零（why？）\n返回分配到的内存地址\n总结 GO中内存布局利用了多级缓存以及隔离适应策略（划分多个size class），优化了内存分配的效率，提高了空间利用率，并且由运行时全权管理堆区内存，方便了开发者。这也正是golang内存占用小、运行速度快、使用简便的原因之一。\n","permalink":"http://localhost:1313/posts/golang/memory/","summary":"介绍了go内存分配的三级模型，还讨论了分配器的种类和设计原则，以及内存布局的组件和对象分配的流程。整体而言，Go的内存分配器通过多级缓存和分级分配的思想，提高了内存管理的效率和空间利用率。","title":"Go 内存分配器"},{"content":"goroutine 参考 操作系统 | 进程/线程切换问题\nDraveness 调度器\nGo 程序员面试笔试宝典 调度器\nGo 特殊的goroutine\n线程和进程和goroutine (进程-\u0026gt;线程-\u0026gt;goroutine) goroutine和线程区别 主要三方面：创建和销毁成本、内存占用、切换成本\n进程 线程 goroutine 创建和销毁 内核级（内存空间、文件描述符等） 内核级 （优化：线程池） 用户级 切换 切换页表刷新TLB（主要影响）、PCB切换， 慢 保存各种寄存器，较慢 只保存三个：PC，SP，BP，快 内存占用 独立的虚拟内存 1MB+“guard page” 初始2KB 进程切换 = 指令序列切换 + 资源切换 线程切换 = 指令序列切换 + 共享资源 为什么线程轻量 同一个进程下，线程之间的共享内存空间（内存占用），所以创建和销毁时只需要关心自己独有的数据，还可以利用线程池进一步减小成本（创建和销毁），进行切换时不需要切换页表（切换）\n为什么goroutine更轻量 同样共享进程内存空间，且栈空间会动态变化，可以减少内存浪费（内存占用），创建和销毁由 Go 运行时（ Go runtime ）负责管理，成本很小是用户级的。（创建和销毁）只需要保存和恢复 Goroutine 的上下文信息，而不需要切换整个线程的上下文（M：N模型），减少了频繁切换的内存开销（切换）\n其实golang做的优化就是：原本每个任务都要对应一个线程，对于一个CPU来说遇到多任务并发执行的情况就要频繁切换线程（解决：等量M），并且需要从全局队列取任务，涉及到加解锁（解决：加P保存本地队列），造成资源的浪费。而 go 中只使用与机器 CPU 数量相等（默认）的线程，将相比线程更轻量的 goroutine 作为具体任务的执行载体，大部分情况下只需要通过处理器 P 和调度器在用户态进行调度，减少了线程切换带来的资源损耗。 go程序执行 go program：用户程序 runtime：runtime 是 Go 的运行时系统，它是一个在程序执行期间负责管理和支持 Go 语言特性的底层系统组件，用于支持 Goroutine 的调度、内存管理、垃圾回收、并发原语等关键功能。 Goroutine 的调度功能就是通过运行时中的 scheduler 实现的。\n调度器 scheduler 在程序运行过程中，runtime.schedt 对象只有一份实体，它负责协调和管理 goroutine 的执行。\n数据结构 runtime.schedt 结构体维护了空闲 M 列表、空闲 P 列表，全局 G 队列、dead G 缓存等等。\n所有 M 所有 P 所有 G 保存在全局变量中（allm, allp, allgs）， 调度器中只是空闲的列表 初始化 一些配置的初始化 获取 g0（ g0 在运行时系统启动时创建） 初始化 m0（初始化id以及gsignal等等） 【加锁】 初始化 GOMAXPROCS 个 P 【解锁】 关于 m0 和 g0 GMP M -\u0026gt; P -\u0026gt; G\nM：Machine，内核线程，即传统意义上的线程 P：Processor，处理器，运行在M上负责本地调度，减缓了全局锁的调用频率 G：Goroutine，待执行任务 G 只存在于 Go 语言的运行时，是用户态线程\n数据结构 runtime.g 结构体中包括使用的栈信息、当前绑定的M（可能为空）、抢占相关信息、调度相关信息等等\n状态 一个 G 最主要的生命周期可以概括为三个状态：\n阻塞中 例如：正在执行系统调用 _Gsyscall，运行时阻塞（channel之类的）_Gwaiting，被抢占 _Gpreempted 可运行 例如：存储在运行队列中 _Grunnable 运行中 例如：被赋予了M和P _Grunnable 此外，还有 GC 相关状态、初始化状态（_Gidle、_Gdead）等。 将空闲的G置为 _Gdead 状态可以避免 GC 扫描未初始化的栈（不知道啥意思，总之是和避免GC干啥有关）\n初始化 获取调用方 G 以及 PC；\n【锁住m0 避免抢占】用 g0 系统栈（systemstack）创建新的 G【解锁m0】（使用系统栈目的是减少栈帧的复制和管理开销，从而提高大型函数调用的性能）：\n获取 g0（启动时创建的）、p0（schedt初始化时创建） 尝试从本地或者全局 dead G 列表中获取空闲的 Goroutine (runtime.p.gFree, runtime.schedt.gFree)，如果没有空闲的则创建一个新的 G 设置其状态为 dead 并加入全局 G 列表 初始化 G 的运行现场以及需要的参数 放到运行队列中（本地p/全局schedt），根据传入的参数 next：\nnext 为 true：将 G 放到处理器的 p.runnext 上作为下一个执行的任务，如果 p.runnext 上原来有 G ，则把他踢到运行队列中； next 为 false：如果本地队列有空间则放本地，否则，就会把本地队列中的一部分 Goroutine 和待加入的 Goroutine 添加到调度器持有的全局运行队列上； 检查空闲的 P，如果有，将其唤醒（提高调度和执行效率）\nM 操作系统线程，调度器最多可以创建 10000 个线程，但是最多只会有当前机器内核数 GOMAXPROCS 个活跃线程能够正常运行。 运行时启动时不会一下创建内核数个M\n数据结构 runtime.m 结构体中包括了调度线程 g0 ，当前运行的goroutine curg 以及一些处理器相关字段、状态信息、调度相关信息等等\n状态 两种：自旋和非自旋 自旋的时候，会努力找工作（检查全局队列，查看 network poller，试图执行 gc 任务，或者“偷”工作）；找不到的时候会进入非自旋状态，之后会休眠，直到有工作需要处理时，被其他工作线程唤醒，又进入自旋状态。 初始化 创建时机：运行时系统启动时（创建m0、创建sysmon线程\u0026hellip;）、调度M去运行P时（没有空闲M则创建新的）\nruntime.newm 为创建新的M的顶层函数，执行了分配内存、初始化部分变量、调用底层函数新建操作系统线程等。\n由汇编语言调用 runtime.mstart 启动线程，初始化PC、SP和signal等运行现场，进入调度循环。\nP P 只是处理器的抽象，而非处理器本身。负责调度本地队列中的goroutine。\n数据结构 runtime.p 结构体反向存储着线程 m ，本地G队列（环形链表）、线程缓存（mcache）以及一些调试、GC 辅助的字段\n状态 一共有五个状态：\n_Pidle：P 处于空闲 _Prunning：被 M 持有，正在执行 _Psyscall：G 陷入系统调用 _Pgcstop：被 M 持有，由于 GC 被停止 _Pdead：由于 GOMAXPROCS shrank，当前 P 不再被使用 初始化 时机：运行时启动时，内核数变化时（很少见） runtime.procresize 负责p的初始化，调用时进入 world stopped 状态；\n根据新的内核数扩容或缩减 allp 全局p列表 扩容：申请新的P对象、做一些初始化操作 缩减：释放被缩减的P所持有的资源，并置其为 _Pdead 状态 判断当前G使用的P是否应该被释放（P在被缩减的范围内），如果是，则将当前P更换为 allp[0]； 遍历全局P列表 如果本地队列为空，则置为 _Pidle 状态，放入 schedt 调度器的空闲P列表 如果本地队列不为空，则绑定一个M，加入可运行的P链表 返回可运行的P链表 调度 触发调度的条件 根据 runtime.schedule 函数的调用方，可分为以下几类：\n线程启动 runtime.mstart 和 Goroutine 执行结束时 内存同步访问时（mutex，channel等阻塞时），主动挂起 系统调用时，G 对应的 M 和 P 也阻塞在系统调用，并不会立刻发生抢占，只有当这个阻塞持续时间过长（10 ms）时，才会将 P（及之上的其他 G）抢占并分配到空闲的 M 上 协作式调度，主动让出P，发生在G时间片耗尽 系统监控、GC等 调度循环 调度函数 runtime.schedule 是一个永不返回的函数，是由每个系统线程M的g0调用的，不断寻找待运行的G并执行，在G结束后会返回调度函数开头开始下一轮循环；\n如何寻找可运行的goroutine（序号为寻找优先级） 为保证公平，每61次调度将首先从调度器（runtime.sched）全局队列中获取G；取1个直接返回。 从p本地队列获取；取runnext，不行再取队首。 从全局队列获取；偷取的G数量最多为本地队列容量的一半，最少为全局队列中的G被每个P平分到的数量。 从netpoll获取； 从其他P偷取； 如果找不到，则将m切换至休眠状态，在切换期间还会一步三回头地再确认一遍确实没有可运行的G了； 步骤1和3都是从全局队列获取，但是步骤3会获取多个G，返回全局队列的队首G，剩下的放到P的本地队列中。 怎么偷 偷取 特指从其他P中获取G。 总共尝试四次，每次都会以随机的顺序遍历所有P，直到偷到。\n前三次会选择不在空闲状态的P（从空闲的P肯定获取不到G），如果队列中有G的话，会将一半的G偷到自己的本地队列里来；\n如果前三次都没偷到，最后一次会先看runnext位置上有没有G，再看本地队列，有就偷一个，还可能窃取处理器的计时器；\n怎么执行 偷到G以后，调用 runtime.execute 让G在M上跑起来\n绑定M和G，将G置为 _Grunning 状态 通过 runtime.gogo 将G调度到当前线程上，是由汇编指令实现的 G执行完成后，转换G状态为 _Gdead，解绑G/M，将G加入gFree列表，切换回g0开始新一轮的调度 总结一下 触发调度会将当前G切换回g0，由g0进行调度操作，待找到待运行的G后，将G切换到当前M上\nTODO 信号处理机制，协作与抢占\n","permalink":"http://localhost:1313/posts/golang/goroutine/","summary":"介绍了goroutine和线程的区别，包括创建销毁成本、内存占用和切换成本。讲述了Go程序的执行过程和调度器的结构及调度过程。","title":"Go 调度器"},{"content":"函数参数值传递 参考：Draveness博客\nslice作为参数 func TestPtr(t *testing.T) { a := []int{1, 2, 3} m := map[int]int{1: 1, 2: 2, 3: 3} t.Log(unsafe.Sizeof(a), unsafe.Sizeof(m)) // 24 8 t.Log(unsafe.Pointer(\u0026amp;a), unsafe.Pointer(\u0026amp;m)) // 0xc000134c78 0xc0001287d0 f := func(sl []int, mp map[int]int) { t.Log(unsafe.Pointer(\u0026amp;sl), unsafe.Pointer(\u0026amp;mp)) // 0xc000134c90 0xc0001287d8 sl[0] = 0 sl = append(sl, 4) // 加不到a上 m[1] = 0 m[4] = 4 } f(a, m) t.Log(a) // [0 2 3] t.Log(m) // map[1:0 2:2 3:3 4:4] } 疑惑：\n为啥修改可以应用到原切片但是不能append？ map为啥在函数内部发生扩容，这个动作也会应用到外部的map中? 原因：\n参数值传递 slice底层结构 map变量实质 参数传递 传递基本类型时，会拷贝其值； 传递切片或数组时，会拷贝切片或数组中的所有值； 传递结构体时：会拷贝结构体中的全部内容； 传递结构体指针时：会拷贝结构体指针的值； slice数据结构 type SliceHeader struct { Data uintptr Len int Cap int } Data 是指向数组的指针; Len 是当前切片的长度； Cap 是当前切片的容量，即 Data 数组的大小： 原因 切片在函数参数中进行传递，相当于传递SliceHeader结构体，会拷贝结构体中的全部内容。\n根据索引进行修改时，会根据Data字段找到底层的数组并修改对应的值；由于函数内的切片sl和函数外的切片a虽然不是同一个SliceHeader结构体，但是其中的全部内容都相同，对data指向的底层数据进行修改会在两个结构体之间共享。\n而append不会影响函数外切片a的原因是，切片的数据是根据Data指向的首地址和Len圈定的范围共同决定的，无论函数内部如何append，外边原切片的Len是不变的，所以确定的底层数组长度也是不变的，添加到数组末尾的元素不在Len圈定的范围之内。\nmap变量实质 因为map、channel这两种类型的值其实是指向runtime.hmap与runtime.hchan的指针，而slice类型就是runtime.sliceHeader类型。\n根据上边代码中这一行输出可以看出：\nt.Log(unsafe.Sizeof(a), unsafe.Sizeof(m)) // 24 8 输出中24等于SliceHeader中三个字段大小之和，而8则对应着一个 uintptr 的大小\n所以slice和map作为形参进行传递分别对应着上文提到的参数传递规则：\n传递结构体时：会拷贝结构体中的全部内容； // slice 传递结构体指针时：会拷贝结构体指针的值； // map ","permalink":"http://localhost:1313/posts/golang/function_paramter_passing/","summary":"这篇文章讨论了Go语言中的参数传递机制，特别是在处理切片和映射时的行为。文章提到参数传递规则、切片结构和共享、修改和添加元素的影响，以及映射的实质。","title":"Go 参数传递机制"},{"content":"map 参考 Draveness博客\n数据结构 runtime/map.go\n// A header for a Go map. type hmap struct { count int // # live cells == size of map. Must be first (used by len() builtin) flags uint8 // 枚举值: 1 2 4 8 B uint8 // log_2 of # of buckets (can hold up to loadFactor * 2^B items) noverflow uint16 // approximate number of overflow buckets; see incrnoverflow for details hash0 uint32 // hash seed buckets unsafe.Pointer // array of 2^B Buckets. may be nil if count==0. oldbuckets unsafe.Pointer // previous bucket array of half the size, non-nil only when growing nevacuate uintptr // progress counter for evacuation (buckets less than this have been evacuated) extra *mapextra // optional fields } // mapextra holds fields that are not present on all maps. type mapextra struct { overflow *[]*bmap oldoverflow *[]*bmap // nextOverflow holds a pointer to a free overflow bucket. nextOverflow *bmap } // runtime.map中定义的结构 A bucket for a Go map. type bmap struct { tophash [bucketCnt]uint8 // 存储了键的哈希的高 8 位 } // 运行期间重构的bmap结构 type bmap struct { topbits [8]uint8 keys [8]keytype values [8]valuetype pad uintptr overflow uintptr } 初始化 根据元素数量和size计算出需要的最小桶数量，条件：低于装载因子（装载因子:=元素数量÷桶数量）\n桶的数量多于 2^4 时，会额外创建 2^(B-4) 个溢出桶\n分配内存空间，正常桶和溢出桶在内存中的存储空间是连续的（在初始化时，后续可能因为扩容而不连续），只是被 hmap 中的不同字段引用（hmap.buckets, hmap.extra.overflow）\n读写操作 访问 如果在扩容过程中（ h.oldbuckets 不为空）并且桶中数据没有被分流（ evacuated ），则下述步骤1.2.3则在 h.oldbuckets， h.extra.oldoverflow 数组中寻找。\n根据 hmap.B bucket的次方数（2^B）计算掩码（比如B=3，则掩码为……0111），通过\u0026amp;运算计算出key哈希值的低B（不是8是B）位，得到对应桶在 hmap.buckets 数组中的索引值，从而可以通过数组首地址和索引值和元素类型三者计算出所求bucket元素（ bmap ）内存地址；\n拿到相应的bucket后，依次遍历正常桶和溢出桶中的 bmap.tophash ，先比较key的高8位和 tophash[i] ，后比较传入的和桶中的值以加速数据的读写（tophash的作用，类似于索引）。\n找到key后，再用 bmap 内存地址加上偏移量找到对应的value值\n写入 首先根据 hmap.flags 判断是否有并发写入，有则panic；并将flags置为写入状态；\n如果map在扩容过程中（ h.oldbuckets 不为空），则需要先进行增量分流（将旧桶分流（如果没被分流过）并额外分流 hmap.nevacuate 对应的桶，即第一个还没被分流的桶）。\n接下来主要逻辑和访问相似，依次遍历正常桶和溢出桶：\n如果遇到相同的 tophash[i] ，则向下寻找对应的key，若key值相同则返回对应的value内存地址。如果不同则跳过（这里也说明tophash数组是可能会有重复值的，但是不会大量重复，因为在选择bukect时是按照哈希低B位分桶的，减少同一个桶中有大量相等 tophash 的概率影响性能。）；\n如果遇到空的tophash[i]，则获取到key和待赋值的value以及tophash[i]的内存地址，并在逻辑中标记状态为已填入，避免重复写入；\n如果所有桶中都没有空位，则创建一个新的溢出桶作为接收的桶；\n在函数最后最后返回value内存地址。而真正的赋值操作是在编译期间插入的。 扩容 什么时候需要扩容 通过 hmap.oldbuckets 是否为空判断map是否在扩容过程中；（map扩容不是一个原子过程）\n满足以下两种情况之一\n装载因子已经超过6.5； -\u0026gt; 翻倍扩容\n哈希使用了太多溢出桶； -\u0026gt; 等量扩容\n问：为什么会等量扩容？\n答：持续向哈希中插入数据并将它们全部删除时，如果哈希表中的数据量没有超过阈值，就会不断积累溢出桶造成缓慢的内存泄漏。\n扩容步骤 空间分配：在写入操作中（步骤2.3之间），如果碰到需要扩容的情况，则将 hmap.buckets, hmap.extra.overflow 中的数据移到 h.oldbuckets， h.extra.oldoverflow 中， 并创建新的正常桶和溢出桶数组（等量或翻倍，因扩容类型不同而异）。这一步骤只是初始化了相应数组，完成初始化后会返回到当前写入操作函数开头的分流函数位置（写入步骤2）；\n增量分流：\n声明 evacDst 结构体 对于翻倍扩容，声明两个 evacDst 结构体保存分配上下文并指向新桶（其实就是保存了下一个分流出的元素应该插入的位置（evacuation destination），包括键/值地址、索引值、新桶地址）；\n对于等量扩容，声明一个 evacDst 结构体；\n遍历正常桶和溢出桶中所有的元素，分流到 evacDst 指向的位置，如果分流桶装满了就新建一个溢出桶；\n如果 h.nevacurate 等于当前桶序号，说明当前序号之前的桶已经分流完了，则更新分流进度（ h.nevacurate++ 直到碰到一个没被分流的旧桶）；\n如果 h.nevacurate 等于旧桶数量，说明map中所有桶都分流完了，所以将 h.oldbuckets， h.extra.oldoverflow 置空释放内存；\n删除 主要逻辑和写入基本相同，遍历找到键/值地址，释放内存，更改tophash[i]的状态。\n","permalink":"http://localhost:1313/posts/golang/map/","summary":"介绍了map的底层结构以及扩容过程","title":"Go map"}]